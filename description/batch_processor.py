import asyncio
import aiohttp
from typing import Any, Dict, List
from .cache import description_cache
from .complexity import calculate_complexity_score
from .ai_client import call_model, call_model_batch


def _generate_unique_key(function_info: Dict[str, Any]) -> str:
    """ç”Ÿæˆå‡½æ•°çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œé¿å…åŒåå‡½æ•°è¦†ç›–
    
    Args:
        function_info: å‡½æ•°ä¿¡æ¯å­—å…¸
        
    Returns:
        å”¯ä¸€æ ‡è¯†ç¬¦å­—ç¬¦ä¸²
    """
    try:
        # è·å–åŸºæœ¬ä¿¡æ¯
        function_name = function_info.get('basic_info', {}).get('function_name', 'unknown')
        file_path = function_info.get('context', {}).get('file_path', '')
        parent_class = function_info.get('context', {}).get('parent_class', {})
        
        # æ„å»ºå”¯ä¸€æ ‡è¯†ç¬?
        key_parts = []
        
        # 1. å‡½æ•°å?
        key_parts.append(function_name)
        
        # 2. æ–‡ä»¶è·¯å¾„ï¼ˆå»é™¤æ‰©å±•åå’Œè·¯å¾„å‰ç¼€ï¼?
        if file_path:
            # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            from pathlib import Path
            file_name = Path(file_path).stem
            if file_name:
                key_parts.append(file_name)
        
        # 3. æ‰€å±ç±»åï¼ˆå¦‚æœæœ‰ï¼‰
        if parent_class and parent_class.get('name'):
            class_name = parent_class['name']
            if class_name:
                key_parts.append(class_name)
        
        # 4. å¦‚æœè¿˜æ˜¯ä¸å¤Ÿå”¯ä¸€ï¼Œæ·»åŠ æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œå€?
        if len(key_parts) < 3:
            path_hash = str(hash(file_path))[-4:]  # å–æœ€å?ä½?
            key_parts.append(f"h{path_hash}")
        
        # ç»„åˆæˆå”¯ä¸€æ ‡è¯†ç¬?
        unique_key = "::".join(key_parts)
        
        # å¦‚æœæ ‡è¯†ç¬¦å¤ªé•¿ï¼Œè¿›è¡Œæˆªæ–­
        if len(unique_key) > 100:
            # ä¿ç•™å‡½æ•°åå’Œæ–‡ä»¶åï¼Œæˆªæ–­å…¶ä»–éƒ¨åˆ†
            if len(key_parts) >= 2:
                unique_key = f"{key_parts[0]}::{key_parts[1]}"
            else:
                unique_key = key_parts[0]
        
        return unique_key
        
    except Exception as e:
        # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›å¸¦å“ˆå¸Œå€¼çš„å‡½æ•°å?
        function_name = function_info.get('basic_info', {}).get('function_name', 'unknown')
        fallback_hash = str(hash(str(function_info)))[-6:]
        return f"{function_name}_fallback_{fallback_hash}"


def _create_function_index(results: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»ºå‡½æ•°ç´¢å¼•ï¼Œä¾¿äºæŸ¥æ‰¾åŒåå‡½æ•?
    
    Args:
        results: å¤„ç†ç»“æœå­—å…¸
        
    Returns:
        ç´¢å¼•å­—å…¸
    """
    try:
        # æŒ‰å‡½æ•°ååˆ†ç»„
        name_groups = {}
        
        for unique_key, result in results.items():
            if 'function_name' in result:
                function_name = result['function_name']
                if function_name not in name_groups:
                    name_groups[function_name] = []
                
                # æ”¶é›†å‡½æ•°ä¿¡æ¯
                func_info = {
                    "unique_key": unique_key,
                    "file_path": result.get('file_path', ''),
                    "has_error": 'error' in result,
                    "complexity_level": result.get('complexity_info', {}).get('complexity_level', 'unknown'),
                    "complexity_score": result.get('complexity_info', {}).get('complexity_score', 0)
                }
                
                # å¦‚æœæœ‰é”™è¯¯ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
                if 'error' in result:
                    func_info["error"] = result['error']
                
                name_groups[function_name].append(func_info)
        
        # åˆ›å»ºç´¢å¼•ç»“æ„
        index = {
            "total_functions": len(results),
            "unique_function_names": len(name_groups),
            "functions_with_duplicates": 0,
            "function_groups": {},
            "duplicate_analysis": {}
        }
        
        # åˆ†ææ¯ä¸ªå‡½æ•°åç»„
        for function_name, instances in name_groups.items():
            count = len(instances)
            
            if count > 1:
                index["functions_with_duplicates"] += 1
                index["duplicate_analysis"][function_name] = {
                    "count": count,
                    "instances": instances
                }
            
            index["function_groups"][function_name] = {
                "count": count,
                "instances": instances
            }
        
        return index
        
    except Exception as e:
        return {
            "error": f"Failed to create index: {str(e)}",
            "total_functions": len(results)
        }


async def generate(functions_list: List[Dict], concurrent: int = 5, batch_size: int = 10, skip_complexity: bool = False) -> Dict[str, Any]:
    """ä½¿ç”¨æ‰¹é‡å¤„ç†ç”Ÿæˆdocstrings
    
    Args:
        functions_list: å‡½æ•°åˆ—è¡¨
        concurrent: å¹¶å‘æ•?
        batch_size: æ‰¹æ¬¡å¤§å°
        skip_complexity: æ˜¯å¦è·³è¿‡å¤æ‚åº¦è®¡ç®—ï¼Œç›´æ¥ä½¿ç”¨moderate
    """
    results: Dict[str, Any] = {}
    sem = asyncio.Semaphore(concurrent)
    processed_count = 0
    total_count = len(functions_list)
    
    # æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è®¡ç®—æ‰€æœ‰å‡½æ•°çš„å¤æ‚åº¦ï¼Œé¿å…é‡å¤è®¡ç®—
    if skip_complexity:
        print("è·³è¿‡å¤æ‚åº¦è®¡ç®—ï¼Œæ‰€æœ‰å‡½æ•°ä½¿ç”¨moderateå¤æ‚åº?..")
        complexity_cache = {}
        simple_functions = []
        complex_functions = []
        
        # æ‰€æœ‰å‡½æ•°éƒ½ä½¿ç”¨moderateå¤æ‚åº?
        for func in functions_list:
            complexity_info = calculate_complexity_score(func, None, skip_calculation=True)
            complexity_cache[id(func)] = complexity_info
            # æ‰€æœ‰å‡½æ•°éƒ½ä½œä¸ºç®€å•å‡½æ•°å¤„ç†ï¼ˆæ‰¹é‡å¤„ç†ï¼?
            simple_functions.append(func)
        
        print(f"æ‰€æœ?{len(simple_functions)} ä¸ªå‡½æ•°å°†ä½¿ç”¨æ‰¹é‡å¤„ç†ï¼ˆmoderateå¤æ‚åº¦ï¼‰")
    else:
        print("æ­£åœ¨é¢„è®¡ç®—å‡½æ•°å¤æ‚åº¦...")
        complexity_cache = {}
        simple_functions = []
        complex_functions = []
        
        # ä½¿ç”¨å¿«é€Ÿå¤æ‚åº¦è®¡ç®—ï¼ˆä¸ä¼ å…¥all_functions_dataé¿å…é‡å¤ç»Ÿè®¡ï¼?
        for func in functions_list:
            complexity_info = calculate_complexity_score(func, None)  # ä¼ å…¥Noneé¿å…é‡å¤è®¡ç®—
            complexity_cache[id(func)] = complexity_info
            
            if complexity_info['complexity_level'] == 'simple':
                simple_functions.append(func)
            else:
                complex_functions.append(func)
        
        print(f"Simple functions: {len(simple_functions)}, Complex functions: {len(complex_functions)}")
    
    async with aiohttp.ClientSession() as session:
        # å¤„ç†ç®€å•å‡½æ•°ï¼ˆæ‰¹é‡ï¼?
        async def process_simple_batch(batch: List[Dict]):
            nonlocal processed_count
            async with sem:
                try:
                    # æ£€æŸ¥ç¼“å­?
                    uncached_batch = []
                    for func in batch:
                        cached_result = description_cache.get(func)
                        if cached_result:
                            unique_key = _generate_unique_key(func)
                            results[unique_key] = cached_result
                            processed_count += 1
                        else:
                            uncached_batch.append(func)
                    
                    if not uncached_batch:
                        return
                    
                    # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„å‡½æ•°
                    batch_results = await call_model_batch(session, uncached_batch, functions_list, complexity_cache)
                    
                    for func, result in zip(uncached_batch, batch_results):
                        unique_key = _generate_unique_key(func)
                        if 'error' not in result:
                            # ç¼“å­˜ç»“æœ
                            description_cache.set(func, result)
                        
                        results[unique_key] = result
                        processed_count += 1
                        # å‡å°‘è¿›åº¦è¾“å‡ºé¢‘ç‡ï¼Œæ¯100ä¸ªå‡½æ•°æ˜¾ç¤ºä¸€æ¬?
                        if processed_count % 100 == 0 or processed_count == total_count:
                            print(f"Progress: {processed_count}/{total_count} - {unique_key}: Batch processed")
                        
                except Exception as e:
                    print(f"Batch processing error: {e}")
                    # å¦‚æœæ‰¹é‡å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å•ç‹¬å¤„ç?
                    for func in batch:
                        unique_key = _generate_unique_key(func)
                        results[unique_key] = {"error": f"Batch processing failed: {str(e)}"}
                        processed_count += 1
        
        # å¤„ç†å¤æ‚å‡½æ•°ï¼ˆå•ç‹¬å¤„ç†ï¼‰
        async def process_complex_function(func: Dict):
            nonlocal processed_count
            async with sem:
                try:
                    # æ£€æŸ¥ç¼“å­?
                    cached_result = description_cache.get(func)
                    if cached_result:
                        unique_key = _generate_unique_key(func)
                        results[unique_key] = cached_result
                        processed_count += 1
                        return
                    
                    # å•ç‹¬å¤„ç†å¤æ‚å‡½æ•°
                    model_response = await call_model(session, "python", func, functions_list, complexity_cache)
                    unique_key = _generate_unique_key(func)
                    
                    # ç¼“å­˜ç»“æœ
                    description_cache.set(func, model_response)
                    
                    results[unique_key] = model_response
                    processed_count += 1
                    # å‡å°‘è¿›åº¦è¾“å‡ºé¢‘ç‡ï¼Œæ¯100ä¸ªå‡½æ•°æ˜¾ç¤ºä¸€æ¬?
                    if processed_count % 100 == 0 or processed_count == total_count:
                        print(f"Progress: {processed_count}/{total_count} - {unique_key}: Complex function processed")
                    
                except Exception as e:
                    unique_key = _generate_unique_key(func)
                    results[unique_key] = {"error": str(e)}
                    processed_count += 1
                    print(f"Progress: {processed_count}/{total_count} - {unique_key}: Error - {str(e)}")
        
        # å¹¶è¡Œå¤„ç†
        tasks = []
        
        # æ‰¹é‡å¤„ç†ç®€å•å‡½æ•?
        for i in range(0, len(simple_functions), batch_size):
            batch = simple_functions[i:i + batch_size]
            tasks.append(process_simple_batch(batch))
        
        # å•ç‹¬å¤„ç†å¤æ‚å‡½æ•°
        for func in complex_functions:
            tasks.append(process_complex_function(func))
        
        await asyncio.gather(*tasks)
    
    return results


def create_complete_data(api_json: Dict[str, Any], description_results: Dict[str, Any], skip_complexity: bool = False) -> Dict[str, Any]:
    """åˆ›å»ºåŒ…å«extract5.pyè¾“å‡ºå’Œdocstringçš„å®Œæ•´æ•°æ?
    
    Args:
        api_json: åŸå§‹çš„extract5.pyè¾“å‡ºæ•°æ®
        description_results: ç”Ÿæˆçš„docstringç»“æœ
        skip_complexity: æ˜¯å¦è·³è¿‡äº†å¤æ‚åº¦è®¡ç®—
        
    Returns:
        åˆå¹¶åçš„å®Œæ•´æ•°æ®
    """
    import time
    
    try:
        # åˆ›å»ºå‡½æ•°é”®æ˜ å°„ï¼Œç”¨äºåŒ¹é…extract5.pyè¾“å‡ºä¸­çš„å‡½æ•°å’Œdescriptionç»“æœ
        def create_function_key(func: Dict[str, Any]) -> str:
            """åˆ›å»ºå‡½æ•°é”®ï¼Œç”¨äºåŒ¹é…ä¸¤ä¸ªJSONæ–‡ä»¶ä¸­çš„å‡½æ•°"""
            func_name = func.get("basic_info", {}).get("function_name", "")
            
            # ä»contextä¸­è·å–file_path
            context = func.get("context", {})
            file_path = context.get("file_path", "")
            
            # ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            if file_path:
                from pathlib import Path
                file_name = Path(file_path).stem
            else:
                file_name = ""
            
            # è·å–çˆ¶ç±»ä¿¡æ¯
            parent_class = context.get("parent_class", {})
            class_name = parent_class.get("name", "") if parent_class else ""
            
            # å¦‚æœå‡½æ•°åœ¨ç±»ä¸­ï¼Œä½¿ç”¨ function_name::file_name::class_name æ ¼å¼
            if class_name:
                return f"{func_name}::{file_name}::{class_name}"
            else:
                return f"{func_name}::{file_name}"
        
        # åˆ›å»ºå®Œæ•´çš„å‡½æ•°æ•°æ?
        complete_functions = []
        functions = api_json.get("functions", [])
        
        for func in functions:
            # åˆ›å»ºå‡½æ•°é”?
            func_key = create_function_key(func)
            
            # æŸ¥æ‰¾å¯¹åº”çš„descriptionç»“æœ
            description_info = description_results.get(func_key, {})
            
            # åˆå¹¶æ•°æ®ï¼Œä¿ç•™extract5.pyçš„æ‰€æœ‰åŸå§‹ä¿¡æ?
            complete_func = {
                # ä¿ç•™åŸå§‹extract5.pyçš„æ‰€æœ‰ä¿¡æ?
                "basic_info": func.get("basic_info", {}),
                "complexity": func.get("complexity", {}),
                "context": func.get("context", {}),
                "importance": func.get("importance", {}),
                
                # æ·»åŠ descriptionä¿¡æ¯
                "description_info": {
                    "docstring": description_info.get("docstring", {}),
                    "complexity_level": description_info.get("complexity_info", {}).get("complexity_level", "unknown"),
                    "context_summary": description_info.get("context_summary", ""),
                    "duration": description_info.get("duration(s)", 0),
                    "has_error": "error" in description_info,
                    "error": description_info.get("error", None)
                }
            }
            
            # å¦‚æœè·³è¿‡äº†å¤æ‚åº¦è®¡ç®—ï¼Œç¡®ä¿å¤æ‚åº¦ä¿¡æ¯æ­£ç¡®è®¾ç½®
            if skip_complexity:
                from .config import DEFAULT_IMPORTANCE
                # è®¾ç½®å¤æ‚åº¦ä¸ºmoderate
                complete_func["description_info"]["complexity_level"] = "moderate"
                # è®¾ç½®é»˜è®¤é‡è¦åº¦ä¸ºmoderate
                if not complete_func.get("importance"):
                    complete_func["importance"] = {
                        "importance_level": DEFAULT_IMPORTANCE,
                        "importance_score": 15.0,  # moderateçš„å…¸å‹åˆ†æ•?
                        "calculation_skipped": True
                    }
            
            complete_functions.append(complete_func)
        
        # åˆ›å»ºå®Œæ•´çš„è¾“å‡ºç»“æ„ï¼Œä¿æŒä¸extract5.pyä¸€è‡´çš„æ ¼å¼
        complete_data = {
            # ä¿ç•™extract5.pyçš„åŸå§‹ç»“æ?
            "repository": api_json.get("repository", ""),
            "analysis_timestamp": api_json.get("analysis_timestamp", ""),
            "total_functions": len(complete_functions),
            "function_names": api_json.get("function_names", []),
            "functions": complete_functions,
            
            # ä¿ç•™extract5.pyçš„ç±»å’Œå‡½æ•°å…³ç³»åˆ†æ?
            "class_function_relationship": api_json.get("class_function_relationship", {}),
            
            # ä¿ç•™extract5.pyçš„APIè°ƒç”¨å…³ç³»åˆ†æ
            "api_call_relationships": api_json.get("api_call_relationships", {}),
            
            # ä¿ç•™extract5.pyçš„ä¼ é€’è°ƒç”¨åˆ†æ?
            "transitive_calls": api_json.get("transitive_calls", {}),
            
            # ä¿ç•™extract5.pyçš„è¿‡æ»¤ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼?
            "filter_info": api_json.get("filter_info", {}),
            
            # æ·»åŠ descriptionç”Ÿæˆä¿¡æ¯
            "description_generation": {
                "source_file": "extract5.py output + description results",
                "total_functions": len(complete_functions),
                "functions_with_docstring": len([f for f in complete_functions if not f["description_info"]["has_error"]]),
                "functions_with_errors": len([f for f in complete_functions if f["description_info"]["has_error"]]),
                "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "complexity_distribution": {}
            }
        }
        
        # æ·»åŠ å¤æ‚åº¦ç»Ÿè®?
        complexity_stats = {"simple": 0, "moderate": 0, "complex": 0, "unknown": 0}
        for func in complete_functions:
            level = func["description_info"]["complexity_level"]
            complexity_stats[level] += 1
        
        complete_data["description_generation"]["complexity_distribution"] = complexity_stats
        
        return complete_data
        
    except Exception as e:
        print(f"Error creating complete data: {e}")
        return {
            "error": f"Failed to create complete data: {str(e)}",
            "metadata": {
                "source_file": "extract5.py output + description results",
                "total_functions": len(api_json.get("functions", [])),
                "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "functions": []
        }
