#!/usr/bin/env python3

import os
import pickle
import hashlib
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import threading
from functools import lru_cache

logger = logging.getLogger(__name__)

class ModelCacheManager:
    """æ¨¡å‹ç¼“å­˜ç®¡ç†å™?- å•ä¾‹æ¨¡å¼"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        self.cache_dir = Path("./model_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # å†…å­˜ç¼“å­˜
        self._memory_cache = {}
        
        # æ¨¡å‹æ–‡ä»¶ç¼“å­˜è·¯å¾„
        self._model_paths = {}
        
        self._initialized = True
        logger.info(f"æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½? {self.cache_dir}")
    
    def get_model_path(self, model_name: str, model_type: str = "sentence_transformers") -> str:
        """è·å–æ¨¡å‹ç¼“å­˜è·¯å¾„"""
        cache_key = f"{model_type}_{model_name}"
        
        if cache_key in self._model_paths:
            return self._model_paths[cache_key]
        
        # ç”Ÿæˆæ¨¡å‹ç¼“å­˜è·¯å¾„
        model_hash = hashlib.md5(f"{model_type}_{model_name}".encode()).hexdigest()[:8]
        model_cache_path = self.cache_dir / f"{model_type}_{model_hash}"
        
        self._model_paths[cache_key] = str(model_cache_path)
        return str(model_cache_path)
    
    def is_model_cached(self, model_name: str, model_type: str = "sentence_transformers") -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜"""
        model_path = self.get_model_path(model_name, model_type)
        return os.path.exists(model_path) and os.path.isdir(model_path)
    
    def get_cached_model(self, model_name: str, model_type: str = "sentence_transformers"):
        """ä»ç¼“å­˜è·å–æ¨¡å?""
        cache_key = f"{model_type}_{model_name}"
        
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­?
        if cache_key in self._memory_cache:
            logger.info(f"ä»å†…å­˜ç¼“å­˜åŠ è½½æ¨¡å? {model_name}")
            return self._memory_cache[cache_key]
        
        # æ£€æŸ¥ç£ç›˜ç¼“å­?
        if self.is_model_cached(model_name, model_type):
            try:
                if model_type == "sentence_transformers":
                    from sentence_transformers import SentenceTransformer
                    model_path = self.get_model_path(model_name, model_type)
                    logger.info(f"ä»ç£ç›˜ç¼“å­˜åŠ è½½æ¨¡å? {model_name}")
                    model = SentenceTransformer(model_path)
                    
                    # ç¼“å­˜åˆ°å†…å­?
                    self._memory_cache[cache_key] = model
                    return model
                    
            except Exception as e:
                logger.warning(f"ä»ç¼“å­˜åŠ è½½æ¨¡å‹å¤±è´? {e}")
                return None
        
        return None
    
    def cache_model(self, model, model_name: str, model_type: str = "sentence_transformers"):
        """ç¼“å­˜æ¨¡å‹"""
        cache_key = f"{model_type}_{model_name}"
        
        # ç¼“å­˜åˆ°å†…å­?
        self._memory_cache[cache_key] = model
        
        # ç¼“å­˜åˆ°ç£ç›˜ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒä¿å­˜ï¼?
        try:
            if hasattr(model, 'save') and model_type == "sentence_transformers":
                model_path = self.get_model_path(model_name, model_type)
                logger.info(f"ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜ç¼“å­? {model_name}")
                model.save(model_path)
        except Exception as e:
            logger.warning(f"ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜å¤±è´? {e}")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._memory_cache.clear()
        logger.info("å†…å­˜ç¼“å­˜å·²æ¸…ç©?)
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        info = {
            "memory_cache_size": len(self._memory_cache),
            "memory_cache_models": list(self._memory_cache.keys()),
            "disk_cache_dir": str(self.cache_dir),
            "disk_cache_size": 0
        }
        
        # è®¡ç®—ç£ç›˜ç¼“å­˜å¤§å°
        if self.cache_dir.exists():
            total_size = 0
            for file_path in self.cache_dir.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
            info["disk_cache_size"] = total_size
        
        return info

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾?
_cache_manager = ModelCacheManager()

def get_model_cache_manager() -> ModelCacheManager:
    """è·å–å…¨å±€æ¨¡å‹ç¼“å­˜ç®¡ç†å™?""
    return _cache_manager

def cached_sentence_transformer(model_name: str = "all-MiniLM-L6-v2"):
    """å¸¦ç¼“å­˜çš„SentenceTransformeråŠ è½½å™?""
    cache_manager = get_model_cache_manager()
    
    # å°è¯•ä»ç¼“å­˜è·å?
    model = cache_manager.get_cached_model(model_name, "sentence_transformers")
    if model is not None:
        return model
    
    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™ä¸‹è½½å¹¶ç¼“å­˜
    try:
        from sentence_transformers import SentenceTransformer
        logger.info(f"æ­£åœ¨ä¸‹è½½å¹¶ç¼“å­˜SentenceTransformeræ¨¡å‹: {model_name}")
        logger.info("é¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        model = SentenceTransformer(model_name)
        cache_manager.cache_model(model, model_name, "sentence_transformers")
        
        logger.info(f"âœ?æ¨¡å‹ä¸‹è½½å¹¶ç¼“å­˜å®Œæˆ? {model_name}")
        return model
        
    except Exception as e:
        logger.error(f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
        raise

@lru_cache(maxsize=1)
def get_default_sentence_transformer():
    """è·å–é»˜è®¤çš„SentenceTransformeræ¨¡å‹ï¼ˆå¸¦LRUç¼“å­˜ï¼?""
    return cached_sentence_transformer("all-MiniLM-L6-v2")

def clear_model_cache():
    """æ¸…ç©ºæ¨¡å‹ç¼“å­˜"""
    cache_manager = get_model_cache_manager()
    cache_manager.clear_cache()
    get_default_sentence_transformer.cache_clear()

def get_cache_status() -> str:
    """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ?""
    cache_manager = get_model_cache_manager()
    info = cache_manager.get_cache_info()
    
    status = []
    status.append("ğŸ“Š æ¨¡å‹ç¼“å­˜çŠ¶æ€?)
    status.append("=" * 40)
    status.append(f"å†…å­˜ç¼“å­˜: {info['memory_cache_size']} ä¸ªæ¨¡å?)
    status.append(f"ç£ç›˜ç¼“å­˜: {info['disk_cache_size']/1024/1024:.2f} MB")
    status.append(f"ç¼“å­˜ç›®å½•: {info['disk_cache_dir']}")
    
    if info['memory_cache_models']:
        status.append("\nå·²ç¼“å­˜çš„æ¨¡å‹:")
        for model in info['memory_cache_models']:
            status.append(f"  - {model}")
    
    return "\n".join(status)

if __name__ == "__main__":
    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹ç¼“å­˜åŠŸèƒ½")
    print("=" * 40)
    
    # è·å–ç¼“å­˜çŠ¶æ€?
    print(get_cache_status())
    
    # æµ‹è¯•åŠ è½½æ¨¡å‹
    print("\nğŸ“¥ æµ‹è¯•åŠ è½½æ¨¡å‹...")
    try:
        model = get_default_sentence_transformer()
        print(f"âœ?æ¨¡å‹åŠ è½½æˆåŠŸ: {type(model).__name__}")
        
        # å†æ¬¡è·å–ç¼“å­˜çŠ¶æ€?
        print("\nğŸ“Š åŠ è½½åçš„ç¼“å­˜çŠ¶æ€?")
        print(get_cache_status())
        
    except Exception as e:
        print(f"â?æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
