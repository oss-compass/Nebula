#!/usr/bin/env python3

import os
import json
import hashlib
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from pathlib import Path
import pickle
import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# å°è¯•å¯¼å…¥å„ç§åµŒå…¥åº?try:
    import sentence_transformers
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import torch
    import transformers
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥æ¨¡å‹ç¼“å­˜ç®¡ç†å™?from .model_cache import get_model_cache_manager, cached_sentence_transformer


@dataclass
class EmbeddingConfig:
    """åµŒå…¥é…ç½®"""
    model_name: str = "all-MiniLM-L6-v2"
    model_type: str = "sentence_transformers"  # sentence_transformers, openai, transformers, tfidf
    dimension: int = 384
    batch_size: int = 32
    max_length: int = 512
    cache_dir: str = "./embedding_cache"
    use_cache: bool = True
    openai_api_key: Optional[str] = None
    openai_model: str = "text-embedding-ada-002"
    device: str = "cpu"


@dataclass
class CodeEmbedding:
    """ä»£ç åµŒå…¥ç»“æœ"""
    id: str
    content: str
    embedding: np.ndarray
    metadata: Dict[str, Any]
    model_name: str
    timestamp: str


class CodeVectorizer:
    """ä»£ç å‘é‡åŒ–å™¨"""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.vectorizer = None
        self.cache_db = None
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(config.cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“
        if config.use_cache:
            self._init_cache_db()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _init_cache_db(self):
        """åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“"""
        cache_db_path = os.path.join(self.config.cache_dir, "embeddings.db")
        self.cache_db = sqlite3.connect(cache_db_path)
        
        # åˆ›å»ºè¡?        self.cache_db.execute("""
            CREATE TABLE IF NOT EXISTS embeddings (
                id TEXT PRIMARY KEY,
                content_hash TEXT,
                embedding BLOB,
                metadata TEXT,
                model_name TEXT,
                timestamp TEXT
            )
        """)
        self.cache_db.commit()
    
    def _load_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        try:
            if self.config.model_type == "sentence_transformers":
                if not SENTENCE_TRANSFORMERS_AVAILABLE:
                    raise ImportError("sentence-transformers not available")
                
                # ä½¿ç”¨æ”¹è¿›çš„ç¼“å­˜ç®¡ç†å™¨
                logger.info(f"æ­£åœ¨åŠ è½½SentenceTransformeræ¨¡å‹: {self.config.model_name}")
                self.model = cached_sentence_transformer(self.config.model_name)
                self.config.dimension = self.model.get_sentence_embedding_dimension()
                
            elif self.config.model_type == "openai":
                if not OPENAI_AVAILABLE:
                    raise ImportError("openai not available")
                if not self.config.openai_api_key:
                    raise ValueError("OpenAI API key required")
                openai.api_key = self.config.openai_api_key
                logger.info("OpenAI API configured")
                
            elif self.config.model_type == "transformers":
                if not TRANSFORMERS_AVAILABLE:
                    raise ImportError("transformers not available")
                self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
                self.model = AutoModel.from_pretrained(self.config.model_name)
                logger.info(f"Loaded Transformers model: {self.config.model_name}")
                
            elif self.config.model_type == "tfidf":
                if not SKLEARN_AVAILABLE:
                    raise ImportError("scikit-learn not available")
                self.vectorizer = TfidfVectorizer(
                    max_features=10000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                logger.info("TF-IDF vectorizer initialized")
                
            else:
                raise ValueError(f"Unsupported model type: {self.config.model_type}")
                
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise
    
    def _get_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _get_cache_key(self, content: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”?""
        content_hash = self._get_content_hash(content)
        return f"{self.config.model_name}_{self.config.model_type}_{content_hash}"
    
    def _get_from_cache(self, content: str) -> Optional[CodeEmbedding]:
        """ä»ç¼“å­˜è·å–åµŒå…?""
        if not self.config.use_cache or not self.cache_db:
            return None
        
        cache_key = self._get_cache_key(content)
        cursor = self.cache_db.execute(
            "SELECT embedding, metadata, timestamp FROM embeddings WHERE id = ?",
            (cache_key,)
        )
        row = cursor.fetchone()
        
        if row:
            embedding_data, metadata_str, timestamp = row
            embedding = pickle.loads(embedding_data)
            metadata = json.loads(metadata_str)
            
            return CodeEmbedding(
                id=cache_key,
                content=content,
                embedding=embedding,
                metadata=metadata,
                model_name=self.config.model_name,
                timestamp=timestamp
            )
        return None
    
    def _save_to_cache(self, embedding: CodeEmbedding):
        """ä¿å­˜åµŒå…¥åˆ°ç¼“å­?""
        if not self.config.use_cache or not self.cache_db:
            return
        
        embedding_data = pickle.dumps(embedding.embedding)
        metadata_str = json.dumps(embedding.metadata)
        
        self.cache_db.execute(
            """INSERT OR REPLACE INTO embeddings 
               (id, content_hash, embedding, metadata, model_name, timestamp)
               VALUES (?, ?, ?, ?, ?, ?)""",
            (
                embedding.id,
                self._get_content_hash(embedding.content),
                embedding_data,
                metadata_str,
                embedding.model_name,
                embedding.timestamp
            )
        )
        self.cache_db.commit()
    
    def _preprocess_code(self, code: str, metadata: Dict[str, Any]) -> str:
        """é¢„å¤„ç†ä»£ç æ–‡æœ?""
        # æå–å…³é”®ä¿¡æ¯
        processed_parts = []
        
        # æ·»åŠ å‡½æ•°å?        if 'name' in metadata:
            processed_parts.append(f"Function: {metadata['name']}")
        
        # æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸?        if 'docstring_description' in metadata and metadata['docstring_description']:
            processed_parts.append(f"Documentation: {metadata['docstring_description']}")
        
        # æ·»åŠ æºä»£ç ?        processed_parts.append(f"Code: {code}")
        
        # æ·»åŠ æ–‡ä»¶è·¯å¾„
        if 'filepath' in metadata:
            processed_parts.append(f"File: {metadata['filepath']}")
        
        # æ·»åŠ ç±»ä¿¡æ?        if 'parent_class_name' in metadata and metadata['parent_class_name']:
            processed_parts.append(f"Class: {metadata['parent_class_name']}")
        
        # æ·»åŠ å‚æ•°ä¿¡æ¯
        if 'parameters_count' in metadata:
            processed_parts.append(f"Parameters: {metadata['parameters_count']}")
        
        # æ·»åŠ è¿”å›ç±»å‹
        if 'return_type' in metadata and metadata['return_type']:
            processed_parts.append(f"Returns: {metadata['return_type']}")
        
        return "\n".join(processed_parts)
    
    def _embed_sentence_transformers(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨ SentenceTransformers ç”ŸæˆåµŒå…¥"""
        embeddings = self.model.encode(
            texts,
            batch_size=self.config.batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        return embeddings
    
    def _embed_openai(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨ OpenAI API ç”ŸæˆåµŒå…¥"""
        embeddings = []
        
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i:i + self.config.batch_size]
            
            response = openai.Embedding.create(
                input=batch,
                model=self.config.openai_model
            )
            
            batch_embeddings = [data['embedding'] for data in response['data']]
            embeddings.extend(batch_embeddings)
        
        return np.array(embeddings)
    
    def _embed_transformers(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨ Transformers ç”ŸæˆåµŒå…¥"""
        embeddings = []
        
        for text in texts:
            # æˆªæ–­æ–‡æœ¬
            if len(text) > self.config.max_length:
                text = text[:self.config.max_length]
            
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                # ä½¿ç”¨ [CLS] token çš„åµŒå…?                embedding = outputs.last_hidden_state[:, 0, :].numpy()
                embeddings.append(embedding[0])
        
        return np.array(embeddings)
    
    def _embed_tfidf(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨ TF-IDF ç”ŸæˆåµŒå…¥"""
        if not hasattr(self, '_fitted_vectorizer') or not self._fitted_vectorizer:
            logger.info(f"Fitting TF-IDF vectorizer with {len(texts)} texts")
            self.vectorizer.fit(texts)
            self._fitted_vectorizer = True
            logger.info("TF-IDF vectorizer fitted successfully")
        
        embeddings = self.vectorizer.transform(texts).toarray()
        return embeddings
    
    def embed_texts(self, texts: List[str], metadata_list: List[Dict[str, Any]] = None) -> List[CodeEmbedding]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        if metadata_list is None:
            metadata_list = [{}] * len(texts)
        
        logger.info(f"embed_texts called with {len(texts)} texts, model_type={self.config.model_type}")
        
        # é¢„å¤„ç†æ–‡æœ?        processed_texts = []
        for i, text in enumerate(texts):
            processed_text = self._preprocess_code(text, metadata_list[i])
            processed_texts.append(processed_text)
        
        # æ£€æŸ¥ç¼“å­?        cached_embeddings = []
        uncached_indices = []
        uncached_texts = []
        uncached_metadata = []
        
        for i, text in enumerate(processed_texts):
            cached = self._get_from_cache(text)
            if cached:
                cached_embeddings.append(cached)
            else:
                uncached_indices.append(i)
                uncached_texts.append(text)
                uncached_metadata.append(metadata_list[i])
        
        # ç”Ÿæˆæœªç¼“å­˜çš„åµŒå…¥
        if uncached_texts:
            logger.info(f"Generating embeddings for {len(uncached_texts)} texts")
            
            if self.config.model_type == "sentence_transformers":
                embeddings_array = self._embed_sentence_transformers(uncached_texts)
            elif self.config.model_type == "openai":
                embeddings_array = self._embed_openai(uncached_texts)
            elif self.config.model_type == "transformers":
                embeddings_array = self._embed_transformers(uncached_texts)
            elif self.config.model_type == "tfidf":
                embeddings_array = self._embed_tfidf(uncached_texts)
            else:
                raise ValueError(f"Unsupported model type: {self.config.model_type}")
            
            # åˆ›å»ºåµŒå…¥å¯¹è±¡
            new_embeddings = []
            for i, (text, metadata, embedding) in enumerate(zip(uncached_texts, uncached_metadata, embeddings_array)):
                cache_key = self._get_cache_key(text)
                embedding_obj = CodeEmbedding(
                    id=cache_key,
                    content=text,
                    embedding=embedding,
                    metadata=metadata,
                    model_name=self.config.model_name,
                    timestamp=str(np.datetime64('now'))
                )
                new_embeddings.append(embedding_obj)
                self._save_to_cache(embedding_obj)
            
            # åˆå¹¶ç»“æœ
            all_embeddings = cached_embeddings + new_embeddings
            
            # æŒ‰åŸå§‹é¡ºåºæ’åº?            result = [None] * len(texts)
            cached_idx = 0
            new_idx = 0
            
            for i in range(len(texts)):
                if i in uncached_indices:
                    result[i] = new_embeddings[new_idx]
                    new_idx += 1
                else:
                    result[i] = cached_embeddings[cached_idx]
                    cached_idx += 1
            
            return result
        
        return cached_embeddings
    
    def embed_single(self, text: str, metadata: Dict[str, Any] = None) -> CodeEmbedding:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…?""
        if metadata is None:
            metadata = {}
        
        return self.embed_texts([text], [metadata])[0]
    
    def embed_text(self, text: str, metadata: Dict[str, Any] = None) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        embedding = self.embed_single(text, metadata)
        return embedding.embedding
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªåµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        # ç¡®ä¿æ˜¯äºŒç»´æ•°ç»?        if embedding1.ndim == 1:
            embedding1 = embedding1.reshape(1, -1)
        if embedding2.ndim == 1:
            embedding2 = embedding2.reshape(1, -1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº?        similarity = np.dot(embedding1, embedding2.T) / (
            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        )
        
        return float(similarity[0, 0])
    
    def find_similar(self, query_embedding: np.ndarray, candidate_embeddings: List[CodeEmbedding], 
                    top_k: int = 10, threshold: float = 0.0) -> List[Tuple[CodeEmbedding, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„åµŒå…?""
        similarities = []
        
        for candidate in candidate_embeddings:
            similarity = self.compute_similarity(query_embedding, candidate.embedding)
            if similarity >= threshold:
                similarities.append((candidate, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]
    
    def save_vectorizer(self, filepath: str):
        """ä¿å­˜å‘é‡åŒ–å™¨çŠ¶æ€?""
        if self.config.model_type == "tfidf" and hasattr(self, '_fitted_vectorizer') and self._fitted_vectorizer:
            import pickle
            with open(filepath, 'wb') as f:
                pickle.dump(self.vectorizer, f)
            logger.info(f"TF-IDF vectorizer saved to {filepath}")
        else:
            logger.warning(f"Cannot save vectorizer: model_type={self.config.model_type}, fitted={getattr(self, '_fitted_vectorizer', False)}")
    
    def load_vectorizer(self, filepath: str):
        """åŠ è½½å‘é‡åŒ–å™¨çŠ¶æ€?""
        if self.config.model_type == "tfidf":
            import pickle
            try:
                with open(filepath, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                self._fitted_vectorizer = True
            except FileNotFoundError:
                logger.warning(f"Vectorizer file not found: {filepath}")
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.cache_db:
            self.cache_db.close()


class CodeEmbeddingManager:
    """ä»£ç åµŒå…¥ç®¡ç†å™?""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.vectorizer = CodeVectorizer(config)
        self.embeddings_index = {}  # id -> CodeEmbedding
        self.embeddings_array = None  # æ‰€æœ‰åµŒå…¥çš„æ•°ç»„
        self.embeddings_ids = []  # å¯¹åº”çš„IDåˆ—è¡¨
    
    def add_embeddings(self, embeddings: List[CodeEmbedding]):
        """æ·»åŠ åµŒå…¥åˆ°ç´¢å¼?""
        for embedding in embeddings:
            self.embeddings_index[embedding.id] = embedding
        
        # é‡å»ºæ•°ç»„ç´¢å¼•
        self._rebuild_array_index()
    
    def _rebuild_array_index(self):
        """é‡å»ºæ•°ç»„ç´¢å¼•"""
        if not self.embeddings_index:
            self.embeddings_array = None
            self.embeddings_ids = []
            return
        
        embeddings_list = list(self.embeddings_index.values())
        self.embeddings_array = np.array([emb.embedding for emb in embeddings_list])
        self.embeddings_ids = [emb.id for emb in embeddings_list]
    
    def search(self, query: str, query_metadata: Dict[str, Any] = None, 
              top_k: int = 10, threshold: float = 0.0) -> List[Tuple[CodeEmbedding, float]]:
        """æœç´¢ç›¸ä¼¼çš„ä»£ç ?""
        if not self.embeddings_index:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.vectorizer.embed_single(query, query_metadata or {})
        
        # è®¡ç®—ç›¸ä¼¼åº?        similarities = []
        for i, candidate_id in enumerate(self.embeddings_ids):
            candidate_embedding = self.embeddings_index[candidate_id]
            similarity = self.vectorizer.compute_similarity(
                query_embedding.embedding, 
                candidate_embedding.embedding
            )
            
            if similarity >= threshold:
                similarities.append((candidate_embedding, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]
    
    def save_index(self, filepath: str):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»?""
        data = {
            'config': self.config.__dict__,
            'embeddings': {
                emb_id: {
                    'id': emb.id,
                    'content': emb.content,
                    'embedding': emb.embedding.tolist(),
                    'metadata': emb.metadata,
                    'model_name': emb.model_name,
                    'timestamp': emb.timestamp
                }
                for emb_id, emb in self.embeddings_index.items()
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load_index(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼?""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # é‡å»ºåµŒå…¥å¯¹è±¡
        self.embeddings_index = {}
        for emb_id, emb_data in data['embeddings'].items():
            embedding = CodeEmbedding(
                id=emb_data['id'],
                content=emb_data['content'],
                embedding=np.array(emb_data['embedding']),
                metadata=emb_data['metadata'],
                model_name=emb_data['model_name'],
                timestamp=emb_data['timestamp']
            )
            self.embeddings_index[emb_id] = embedding
        
        # é‡å»ºæ•°ç»„ç´¢å¼•
        self._rebuild_array_index()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if not self.embeddings_index:
            return {'total_embeddings': 0}
        
        embeddings = list(self.embeddings_index.values())
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_embeddings': len(embeddings),
            'model_name': self.config.model_name,
            'model_type': self.config.model_type,
            'dimension': self.config.dimension,
            'file_types': {},
            'function_types': {},
            'complexity_distribution': {'low': 0, 'medium': 0, 'high': 0, 'very_high': 0}
        }
        
        for emb in embeddings:
            metadata = emb.metadata
            
            # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            file_type = metadata.get('file_type', 'unknown')
            stats['file_types'][file_type] = stats['file_types'].get(file_type, 0) + 1
            
            # å‡½æ•°ç±»å‹ç»Ÿè®¡
            function_type = metadata.get('function_type', 'unknown')
            stats['function_types'][function_type] = stats['function_types'].get(function_type, 0) + 1
            
            # å¤æ‚åº¦åˆ†å¸?            complexity_score = metadata.get('complexity_score', 0)
            if complexity_score < 2:
                stats['complexity_distribution']['low'] += 1
            elif complexity_score < 5:
                stats['complexity_distribution']['medium'] += 1
            elif complexity_score < 10:
                stats['complexity_distribution']['high'] += 1
            else:
                stats['complexity_distribution']['very_high'] += 1
        
        return stats
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.vectorizer:
            self.vectorizer.close()


def create_default_config() -> EmbeddingConfig:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return EmbeddingConfig(
        model_name="all-MiniLM-L6-v2",
        model_type="sentence_transformers",
        dimension=384,
        batch_size=32,
        max_length=512,
        cache_dir="./embedding_cache",
        use_cache=True
    )


def create_openai_config(api_key: str) -> EmbeddingConfig:
    """åˆ›å»º OpenAI é…ç½®"""
    return EmbeddingConfig(
        model_name="text-embedding-ada-002",
        model_type="openai",
        dimension=1536,
        batch_size=100,
        max_length=8191,
        cache_dir="./embedding_cache",
        use_cache=True,
        openai_api_key=api_key,
        openai_model="text-embedding-ada-002"
    )


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    config = create_default_config()
    manager = CodeEmbeddingManager(config)
    
    # æµ‹è¯•æ•°æ®
    test_codes = [
        "def calculate_fibonacci(n):\n    if n <= 1:\n        return n\n    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)",
        "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1"
    ]
    
    test_metadata = [
        {"name": "calculate_fibonacci", "filepath": "math.py", "function_type": "recursive"},
        {"name": "binary_search", "filepath": "search.py", "function_type": "algorithm"}
    ]
    
    # ç”ŸæˆåµŒå…¥
    embeddings = manager.vectorizer.embed_texts(test_codes, test_metadata)
    manager.add_embeddings(embeddings)
    
    # æœç´¢æµ‹è¯•
    query = "find element in sorted array"
    results = manager.search(query, top_k=2)
    
    print(f"Query: {query}")
    print("Results:")
    for embedding, similarity in results:
        print(f"  Similarity: {similarity:.4f}")
        print(f"  Function: {embedding.metadata.get('name', 'Unknown')}")
        print(f"  File: {embedding.metadata.get('filepath', 'Unknown')}")
        print()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    print("Index Statistics:")
    print(json.dumps(stats, indent=2))
    
    manager.close()
