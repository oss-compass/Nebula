import logging
import json
import re
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from collections import defaultdict
from dataclasses import dataclass

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers not available, semantic search will be disabled")

try:
    import jieba
    import jieba.analyse
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logging.warning("jieba not available, keyword extraction will be limited")

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("openai not available, AI-enhanced search will be disabled")

from .vector_embedding import CodeVectorizer, EmbeddingConfig, CodeEmbedding
from .sync_indexer import SyncSemanticIndexer, SyncIndexerConfig

logger = logging.getLogger(__name__)

@dataclass
class SearchQuery:
    """æœç´¢æŸ¥è¯¢"""
    query: str
    query_type: str = "semantic"  # semantic, keyword, hybrid, ai
    top_k: int = 10
    threshold: float = 0.7
    filters: Optional[Dict[str, Any]] = None

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    code_embedding: CodeEmbedding
    similarity_score: float
    explanation: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class SearchConfig:
    """æœç´¢é…ç½®"""
    embedding_model: str = "all-MiniLM-L6-v2"
    similarity_threshold: float = 0.7
    max_results: int = 50
    enable_cache: bool = True
    enable_ai_enhancement: bool = False
    openai_api_key: Optional[str] = None
    recommendation_threshold: float = 0.7
    max_recommendations: int = 5

class SingleRepoSearch:
    """å•åº“è¯­ä¹‰æœç´¢å™?""
    
    def __init__(self, 
                 neo4j_uri: Optional[str] = None,
                 neo4j_user: Optional[str] = None,
                 neo4j_password: Optional[str] = None,
                 neo4j_database: Optional[str] = None,
                 config: Optional[SearchConfig] = None):
        """
        åˆå§‹åŒ–å•åº“æœç´¢å™¨
        
        Args:
            neo4j_uri: Neo4jæ•°æ®åº“URI
            neo4j_user: Neo4jç”¨æˆ·å?            neo4j_password: Neo4jå¯†ç 
            neo4j_database: æ•°æ®åº“åç§?            config: æœç´¢é…ç½®
        """
        self.config = config or SearchConfig()
        self.search_cache = {}
        self.search_history = []
        
        # åˆå§‹åŒ–Neo4jè¿æ¥
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.neo4j_database = neo4j_database
        
        # åˆå§‹åŒ–å‘é‡åŒ–å™?        self.vectorizer = None
        self._init_vectorizer()
        
        # åˆå§‹åŒ–ç´¢å¼•å™¨
        self.indexer = None
        self._init_indexer()
        
        logger.info("å•åº“è¯­ä¹‰æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_vectorizer(self):
        """åˆå§‹åŒ–å‘é‡åŒ–å™?""
        try:
            embedding_config = EmbeddingConfig(
                model_name=self.config.embedding_model,
                device="cpu"
            )
            self.vectorizer = CodeVectorizer(embedding_config)
            logger.info(f"å‘é‡åŒ–å™¨åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {self.config.embedding_model}")
        except Exception as e:
            logger.error(f"å‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´? {e}")
            self.vectorizer = None
    
    def _init_indexer(self):
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        try:
            if self.neo4j_uri:
                # ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºç´¢å¼•å™?                from .sync_indexer import create_default_sync_config
                indexer_config = create_default_sync_config(self.neo4j_uri)
                self.indexer = SyncSemanticIndexer(indexer_config)
                logger.info("ç´¢å¼•å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("æœªæä¾›Neo4jé…ç½®ï¼Œç´¢å¼•å™¨æœªåˆå§‹åŒ–")
        except Exception as e:
            logger.error(f"ç´¢å¼•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.indexer = None
    
    def search(self, query: SearchQuery) -> List[SearchResult]:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"æ‰§è¡Œæœç´¢: {query.query} (ç±»å‹: {query.query_type})")
        
        # æ£€æŸ¥ç¼“å­?        cache_key = self._get_cache_key(query)
        if self.config.enable_cache and cache_key in self.search_cache:
            logger.info("è¿”å›ç¼“å­˜ç»“æœ")
            return self.search_cache[cache_key]
        
        # æ‰§è¡Œæœç´¢
        if query.query_type == "semantic":
            results = self._semantic_search(query)
        elif query.query_type == "keyword":
            results = self._keyword_search(query)
        elif query.query_type == "hybrid":
            results = self._hybrid_search(query)
        elif query.query_type == "ai":
            results = self._ai_enhanced_search(query)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸ¥è¯¢ç±»å‹: {query.query_type}")
        
        # åå¤„ç†ç»“æ?        processed_results = self._post_process_results(results, query)
        
        # ç¼“å­˜ç»“æœ
        if self.config.enable_cache:
            self._cache_results(cache_key, processed_results)
        
        # è®°å½•æœç´¢å†å²
        self._record_search(query, processed_results)
        
        return processed_results
    
    def search_by_natural_language(self, 
                                  query: str, 
                                  limit: int = 10,
                                  search_type: str = "hybrid",
                                  similarity_threshold: float = None) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è‡ªç„¶è¯­è¨€æœç´¢ä»£ç 
        
        Args:
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            search_type: æœç´¢ç±»å‹ ("semantic", "keyword", "hybrid", "ai")
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€?            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        search_query = SearchQuery(
            query=query,
            query_type=search_type,
            top_k=limit,
            threshold=similarity_threshold or self.config.similarity_threshold
        )
        
        results = self.search(search_query)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼?        return [self._result_to_dict(result) for result in results]
    
    def search_similar_functions(self, 
                               function_name: str,
                               limit: int = 10,
                               similarity_threshold: float = None) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼å‡½æ•°
        
        Args:
            function_name: å‡½æ•°å?            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€?            
        Returns:
            ç›¸ä¼¼å‡½æ•°åˆ—è¡¨
        """
        logger.info(f"æŸ¥æ‰¾ç›¸ä¼¼å‡½æ•°: {function_name}")
        
        # é¦–å…ˆè·å–ç›®æ ‡å‡½æ•°çš„ä¿¡æ?        target_function = self._get_function_info(function_name)
        if not target_function:
            logger.warning(f"æœªæ‰¾åˆ°å‡½æ•? {function_name}")
            return []
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = SearchQuery(
            query=f"function similar to {function_name}",
            query_type="semantic",
            top_k=limit,
            threshold=similarity_threshold or self.config.similarity_threshold,
            filters={"exclude_function": function_name}
        )
        
        results = self.search(search_query)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼?        return [self._result_to_dict(result) for result in results]
    
    def search_by_complexity(self, 
                           complexity_level: Optional[str] = None,
                           min_lines: Optional[int] = None,
                           max_lines: Optional[int] = None,
                           min_complexity: Optional[float] = None,
                           max_complexity: Optional[float] = None,
                           limit: int = 100) -> List[Dict[str, Any]]:
        """
        æŒ‰å¤æ‚åº¦æœç´¢å‡½æ•°
        
        Args:
            complexity_level: å¤æ‚åº¦çº§åˆ?("low", "medium", "high")
            min_lines: æœ€å°è¡Œæ•?            max_lines: æœ€å¤§è¡Œæ•?            min_complexity: æœ€å°å¤æ‚åº¦
            max_complexity: æœ€å¤§å¤æ‚åº¦
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"æŒ‰å¤æ‚åº¦æœç´¢: {complexity_level}")
        
        # æ„å»ºå¤æ‚åº¦è¿‡æ»¤å™¨
        filters = {}
        if complexity_level:
            filters["complexity_level"] = complexity_level
        if min_lines is not None:
            filters["min_lines"] = min_lines
        if max_lines is not None:
            filters["max_lines"] = max_lines
        if min_complexity is not None:
            filters["min_complexity"] = min_complexity
        if max_complexity is not None:
            filters["max_complexity"] = max_complexity
        
        search_query = SearchQuery(
            query="complex functions",
            query_type="keyword",
            top_k=limit,
            filters=filters
        )
        
        results = self.search(search_query)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼?        return [self._result_to_dict(result) for result in results]
    
    def _semantic_search(self, query: SearchQuery) -> List[SearchResult]:
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        if not self.vectorizer:
            logger.error("å‘é‡åŒ–å™¨æœªåˆå§‹åŒ–")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.vectorizer.embed_text(query.query)
            
            # ä»ç´¢å¼•å™¨è·å–å€™é€‰ç»“æ?            candidates = self._get_candidates_from_indexer(query)
            
            # å¦‚æœæ²¡æœ‰ç´¢å¼•å™¨æˆ–æ²¡æœ‰å€™é€‰ç»“æœï¼Œä½¿ç”¨é»˜è®¤çš„ä»£ç ç¤ºä¾?            if not candidates:
                logger.warning("ç´¢å¼•å™¨æœªåˆå§‹åŒ–æˆ–æ— å€™é€‰ç»“æœï¼Œä½¿ç”¨é»˜è®¤ä»£ç ç¤ºä¾‹")
                candidates = self._get_default_code_samples()
            
            # è®¡ç®—ç›¸ä¼¼åº?            results = []
            for candidate in candidates:
                if hasattr(candidate, 'embedding') and candidate.embedding is not None:
                    similarity = self._calculate_similarity(query_embedding, candidate.embedding)
                    if similarity >= query.threshold:
                        result = SearchResult(
                            code_embedding=candidate,
                            similarity_score=similarity,
                            explanation=f"è¯­ä¹‰ç›¸ä¼¼åº? {similarity:.3f}"
                        )
                        results.append(result)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            return results[:query.top_k]
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _keyword_search(self, query: SearchQuery) -> List[SearchResult]:
        """æ‰§è¡Œå…³é”®è¯æœç´?""
        try:
            # æå–å…³é”®è¯?            keywords = self._extract_keywords(query.query)
            
            # ä»ç´¢å¼•å™¨æœç´¢
            candidates = self._search_by_keywords(keywords, query)
            
            # è½¬æ¢ä¸ºæœç´¢ç»“æ?            results = []
            for candidate in candidates:
                result = SearchResult(
                    code_embedding=candidate,
                    similarity_score=1.0,  # å…³é”®è¯åŒ¹é…ç»™æ»¡åˆ†
                    explanation="å…³é”®è¯åŒ¹é…?
                )
                results.append(result)
            
            return results[:query.top_k]
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´? {e}")
            return []
    
    def _hybrid_search(self, query: SearchQuery) -> List[SearchResult]:
        """æ‰§è¡Œæ··åˆæœç´¢"""
        try:
            # æ‰§è¡Œè¯­ä¹‰æœç´¢
            semantic_results = self._semantic_search(query)
            
            # æ‰§è¡Œå…³é”®è¯æœç´?            keyword_results = self._keyword_search(query)
            
            # åˆå¹¶ç»“æœ
            all_results = semantic_results + keyword_results
            
            # å»é‡å¹¶é‡æ–°æ’åº?            unique_results = self._deduplicate_results(all_results)
            unique_results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            return unique_results[:query.top_k]
            
        except Exception as e:
            logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            return []
    
    def _ai_enhanced_search(self, query: SearchQuery) -> List[SearchResult]:
        """æ‰§è¡ŒAIå¢å¼ºæœç´¢"""
        if not OPENAI_AVAILABLE or not self.config.openai_api_key:
            logger.warning("OpenAIä¸å¯ç”¨ï¼Œå›é€€åˆ°æ··åˆæœç´?)
            return self._hybrid_search(query)
        
        try:
            # é¦–å…ˆæ‰§è¡Œæ··åˆæœç´¢
            base_results = self._hybrid_search(query)
            
            # ä½¿ç”¨AIé‡æ–°æ’åºå’Œè§£é‡?            enhanced_results = self._ai_enhance_results(query, base_results)
            
            return enhanced_results
            
        except Exception as e:
            logger.error(f"AIå¢å¼ºæœç´¢å¤±è´¥: {e}")
            return self._hybrid_search(query)
    
    def _get_candidates_from_indexer(self, query: SearchQuery) -> List[CodeEmbedding]:
        """ä»ç´¢å¼•å™¨è·å–å€™é€‰ç»“æ?""
        if not self.indexer:
            logger.warning("ç´¢å¼•å™¨æœªåˆå§‹åŒ?)
            return []
        
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ç´¢å¼•å™¨æ¥å£è°ƒæ•?            # å‡è®¾ç´¢å¼•å™¨æœ‰searchæ–¹æ³•
            if hasattr(self.indexer, 'search'):
                return self.indexer.search(query.query, limit=query.top_k * 2)
            else:
                logger.warning("ç´¢å¼•å™¨ä¸æ”¯æŒæœç´¢åŠŸèƒ½")
                return []
        except Exception as e:
            logger.error(f"ä»ç´¢å¼•å™¨è·å–å€™é€‰ç»“æœå¤±è´? {e}")
            return []
    
    def _get_default_code_samples(self) -> List[CodeEmbedding]:
        """è·å–é»˜è®¤ä»£ç ç¤ºä¾‹ï¼ˆå½“æ²¡æœ‰ç´¢å¼•å™¨æ—¶ä½¿ç”¨ï¼?""
        default_samples = [
            "def calculate_sum(a, b): return a + b",
            "def multiply_numbers(x, y): return x * y",
            "def find_max_value(numbers): return max(numbers)",
            "def process_user_input(data): return data.strip().lower()",
            "def validate_email(email): return '@' in email and '.' in email",
            "def connect_to_database(host, port): return database.connect(host, port)",
            "def handle_error(error): print(f'Error: {error}')",
            "def save_to_file(filename, content): open(filename, 'w').write(content)",
            "def read_file(filename): return open(filename, 'r').read()",
            "def sort_list(items): return sorted(items)",
            "def filter_data(data, condition): return [item for item in data if condition(item)]",
            "def format_string(template, **kwargs): return template.format(**kwargs)"
        ]
        
        # ä¸ºæ¯ä¸ªç¤ºä¾‹ç”ŸæˆåµŒå…¥å‘é‡?        embeddings = self.vectorizer.embed_texts(default_samples)
        
        # è½¬æ¢ä¸ºCodeEmbeddingå¯¹è±¡
        code_embeddings = []
        for i, (sample, embedding) in enumerate(zip(default_samples, embeddings)):
            code_embedding = CodeEmbedding(
                id=f"default_sample_{i}",
                content=sample,
                embedding=embedding.embedding,
                metadata={
                    "file_path": "default_samples.py",
                    "line_number": i + 1,
                    "function_name": sample.split('(')[0].replace('def ', '') if 'def ' in sample else f"sample_{i}",
                    "class_name": None,
                    "docstring": None,
                    "complexity_score": 1.0
                },
                model_name=self.vectorizer.config.model_name,
                timestamp="2024-01-01T00:00:00Z"
            )
            code_embeddings.append(code_embedding)
        
        logger.info(f"ç”Ÿæˆäº?{len(code_embeddings)} ä¸ªé»˜è®¤ä»£ç ç¤ºä¾?)
        return code_embeddings
    
    def _search_by_keywords(self, keywords: List[str], query: SearchQuery) -> List[CodeEmbedding]:
        """æ ¹æ®å…³é”®è¯æœç´?""
        if not self.indexer:
            return []
        
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ç´¢å¼•å™¨æ¥å£è°ƒæ•?            # å‡è®¾ç´¢å¼•å™¨æœ‰keyword_searchæ–¹æ³•
            if hasattr(self.indexer, 'keyword_search'):
                return self.indexer.keyword_search(keywords, limit=query.top_k)
            else:
                logger.warning("ç´¢å¼•å™¨ä¸æ”¯æŒå…³é”®è¯æœç´?)
                return []
        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´? {e}")
            return []
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯?""
        if JIEBA_AVAILABLE:
            # ä½¿ç”¨jiebaæå–å…³é”®è¯?            keywords = jieba.analyse.extract_tags(text, topK=10)
            return keywords
        else:
            # ç®€å•çš„å…³é”®è¯æå?            words = re.findall(r'\b\w+\b', text.lower())
            return list(set(words))
    
    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦"""
        try:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº?            dot_product = np.dot(embedding1, embedding2)
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´? {e}")
            return 0.0
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """å»é‡æœç´¢ç»“æœ"""
        seen = set()
        unique_results = []
        
        for result in results:
            # ä½¿ç”¨ä»£ç å†…å®¹çš„å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
            content_hash = hash(result.code_embedding.content)
            if content_hash not in seen:
                seen.add(content_hash)
                unique_results.append(result)
        
        return unique_results
    
    def _ai_enhance_results(self, query: SearchQuery, results: List[SearchResult]) -> List[SearchResult]:
        """ä½¿ç”¨AIå¢å¼ºæœç´¢ç»“æœ"""
        if not OPENAI_AVAILABLE or not self.config.openai_api_key:
            return results
        
        try:
            openai.api_key = self.config.openai_api_key
            
            # æ„å»ºæç¤º
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹ä»£ç æœç´¢ç»“æœï¼Œå¹¶é‡æ–°æ’åºå’Œè§£é‡Šï¼?            
            æŸ¥è¯¢: {query.query}
            
            æœç´¢ç»“æœ:
            """
            
            for i, result in enumerate(results[:5]):  # åªå¤„ç†å‰5ä¸ªç»“æ?                prompt += f"\n{i+1}. {result.code_embedding.content[:200]}..."
            
            prompt += "\n\nè¯·æä¾›é‡æ–°æ’åºçš„å»ºè®®å’Œæ¯ä¸ªç»“æœçš„è§£é‡Šã€?
            
            # è°ƒç”¨OpenAI API
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000
            )
            
            # è§£æå“åº”å¹¶æ›´æ–°ç»“æ?            ai_explanation = response.choices[0].message.content
            
            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ AIè§£é‡Š
            for result in results:
                result.explanation = ai_explanation
            
            return results
            
        except Exception as e:
            logger.error(f"AIå¢å¼ºå¤±è´¥: {e}")
            return results
    
    def _post_process_results(self, results: List[SearchResult], query: SearchQuery) -> List[SearchResult]:
        """åå¤„ç†æœç´¢ç»“æ?""
        # åº”ç”¨è¿‡æ»¤å™?        if query.filters:
            results = self._apply_filters(results, query.filters)
        
        # é™åˆ¶ç»“æœæ•°é‡
        results = results[:query.top_k]
        
        return results
    
    def _apply_filters(self, results: List[SearchResult], filters: Dict[str, Any]) -> List[SearchResult]:
        """åº”ç”¨è¿‡æ»¤å™?""
        filtered_results = []
        
        for result in results:
            if self._matches_filters(result, filters):
                filtered_results.append(result)
        
        return filtered_results
    
    def _matches_filters(self, result: SearchResult, filters: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦åŒ¹é…è¿‡æ»¤å™¨"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„CodeEmbeddingç»“æ„æ¥å®ç?        # å‡è®¾CodeEmbeddingæœ‰metadataå±æ€?        if not hasattr(result.code_embedding, 'metadata'):
            return True
        
        metadata = result.code_embedding.metadata
        
        for key, value in filters.items():
            if key == "exclude_function":
                if metadata.get("function_name") == value:
                    return False
            elif key == "complexity_level":
                if metadata.get("complexity_level") != value:
                    return False
            elif key == "min_lines":
                if metadata.get("lines", 0) < value:
                    return False
            elif key == "max_lines":
                if metadata.get("lines", 0) > value:
                    return False
            elif key == "min_complexity":
                if metadata.get("complexity", 0) < value:
                    return False
            elif key == "max_complexity":
                if metadata.get("complexity", 0) > value:
                    return False
        
        return True
    
    def _get_function_info(self, function_name: str) -> Optional[Dict[str, Any]]:
        """è·å–å‡½æ•°ä¿¡æ¯"""
        if not self.indexer:
            return None
        
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ç´¢å¼•å™¨æ¥å£è°ƒæ•?            if hasattr(self.indexer, 'get_function_info'):
                return self.indexer.get_function_info(function_name)
            else:
                logger.warning("ç´¢å¼•å™¨ä¸æ”¯æŒè·å–å‡½æ•°ä¿¡æ¯")
                return None
        except Exception as e:
            logger.error(f"è·å–å‡½æ•°ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _result_to_dict(self, result: SearchResult) -> Dict[str, Any]:
        """å°†æœç´¢ç»“æœè½¬æ¢ä¸ºå­—å…¸"""
        # ä»metadataä¸­è·å–å‡½æ•°åï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»contentä¸­æå?        function_name = 'Unknown'
        file_path = 'Unknown'
        line_number = 0
        
        if hasattr(result.code_embedding, 'metadata') and result.code_embedding.metadata:
            function_name = result.code_embedding.metadata.get('function_name', 'Unknown')
            file_path = result.code_embedding.metadata.get('file_path', 'Unknown')
            line_number = result.code_embedding.metadata.get('line_number', 0)
        
        # å¦‚æœmetadataä¸­æ²¡æœ‰å‡½æ•°åï¼Œå°è¯•ä»contentä¸­æå?        if function_name == 'Unknown' and result.code_embedding.content:
            import re
            match = re.search(r'def\s+(\w+)', result.code_embedding.content)
            if match:
                function_name = match.group(1)
        
        return {
            "name": function_name,
            "content": result.code_embedding.content,
            "similarity_score": result.similarity_score,
            "explanation": result.explanation,
            "metadata": result.metadata or {},
            "file_path": file_path,
            "line_number": line_number
        }
    
    def _get_cache_key(self, query: SearchQuery) -> str:
        """ç”Ÿæˆç¼“å­˜é”?""
        import hashlib
        key_data = f"{query.query}_{query.query_type}_{query.top_k}_{query.threshold}_{json.dumps(query.filters or {}, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _cache_results(self, cache_key: str, results: List[SearchResult]):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        self.search_cache[cache_key] = results
    
    def _record_search(self, query: SearchQuery, results: List[SearchResult]):
        """è®°å½•æœç´¢å†å²"""
        search_record = {
            "timestamp": json.dumps({"timestamp": "now"}),
            "query": query.query,
            "query_type": query.query_type,
            "result_count": len(results)
        }
        self.search_history.append(search_record)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.search_history) > 100:
            self.search_history = self.search_history[-100:]
    
    def get_search_history(self) -> List[Dict[str, Any]]:
        """è·å–æœç´¢å†å²"""
        return self.search_history.copy()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.search_cache.clear()
        logger.info("æœç´¢ç¼“å­˜å·²æ¸…ç©?)
    
    def close(self):
        """å…³é—­æœç´¢å™?""
        if self.indexer:
            self.indexer.close()
        logger.info("å•åº“æœç´¢å™¨å·²å…³é—­")

def create_single_repo_search(neo4j_uri: str = None,
                             neo4j_user: str = None,
                             neo4j_password: str = None,
                             neo4j_database: str = None,
                             config: SearchConfig = None) -> SingleRepoSearch:
    """åˆ›å»ºå•åº“æœç´¢å™¨å®ä¾?""
    return SingleRepoSearch(
        neo4j_uri=neo4j_uri,
        neo4j_user=neo4j_user,
        neo4j_password=neo4j_password,
        neo4j_database=neo4j_database,
        config=config
    )
