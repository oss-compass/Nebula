import logging
import json
import re
from typing import Dict, List, Any, Optional, Tuple, Set
import numpy as np
from collections import defaultdict, Counter
from dataclasses import dataclass
import concurrent.futures
from threading import Lock
from datetime import datetime

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers not available, semantic analysis will be disabled")

from .vector_embedding import CodeVectorizer, EmbeddingConfig, CodeEmbedding
from .multi_repo import MultiRepoSearch, RepoConfig, MultiRepoSearchConfig

logger = logging.getLogger(__name__)

@dataclass
class APISimilarityConfig:
    """APIç›¸ä¼¼æ€§åˆ†æé…ç½?""
    embedding_model: str = "all-MiniLM-L6-v2"
    similarity_threshold: float = 0.8
    semantic_similarity_threshold: float = 0.7
    structural_similarity_threshold: float = 0.6
    max_apis_per_repo: int = 1000
    enable_parallel_analysis: bool = True
    max_workers: int = 4
    enable_cache: bool = True

@dataclass
class APISignature:
    """APIç­¾å"""
    name: str
    parameters: List[str]
    return_type: Optional[str]
    docstring: Optional[str]
    file_path: str
    line_number: int
    repo_name: str

@dataclass
class SimilarityResult:
    """ç›¸ä¼¼æ€§åˆ†æç»“æ?""
    api1: APISignature
    api2: APISignature
    semantic_similarity: float
    structural_similarity: float
    overall_similarity: float
    similarity_type: str  # "exact", "semantic", "structural", "hybrid"
    explanation: str

@dataclass
class APIAnalysisResult:
    """APIåˆ†æç»“æœ"""
    api_name: str
    total_occurrences: int
    repos_containing: List[str]
    similarity_groups: List[List[APISignature]]
    usage_patterns: Dict[str, Any]
    evolution_analysis: Optional[Dict[str, Any]] = None

class APISimilarityAnalyzer:
    """APIç›¸ä¼¼æ€§åˆ†æå™¨"""
    
    def __init__(self, 
                 multi_repo_search: MultiRepoSearch,
                 config: Optional[APISimilarityConfig] = None):
        """
        åˆå§‹åŒ–APIç›¸ä¼¼æ€§åˆ†æå™¨
        
        Args:
            multi_repo_search: å¤šåº“æœç´¢å™¨å®ä¾?
            config: åˆ†æé…ç½®
        """
        self.multi_repo_search = multi_repo_search
        self.config = config or APISimilarityConfig()
        self.analysis_cache = {}
        self.analysis_history = []
        self.cache_lock = Lock()
        
        # åˆå§‹åŒ–å‘é‡åŒ–å™?
        self.vectorizer = None
        self._init_vectorizer()
        
        logger.info("APIç›¸ä¼¼æ€§åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ?)
    
    def _init_vectorizer(self):
        """åˆå§‹åŒ–å‘é‡åŒ–å™?""
        try:
            embedding_config = EmbeddingConfig(
                model_name=self.config.embedding_model,
                device="cpu"
            )
            self.vectorizer = CodeVectorizer(embedding_config)
            logger.info(f"å‘é‡åŒ–å™¨åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {self.config.embedding_model}")
        except Exception as e:
            logger.error(f"å‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´? {e}")
            self.vectorizer = None
    
    def analyze_api_similarity(self, 
                             api_name: str,
                             similarity_threshold: float = None,
                             analysis_type: str = "comprehensive") -> APIAnalysisResult:
        """
        åˆ†æAPIç›¸ä¼¼æ€?
        
        Args:
            api_name: APIåç§°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€?
            analysis_type: åˆ†æç±»å‹ ("comprehensive", "semantic", "structural")
            
        Returns:
            APIåˆ†æç»“æœ
        """
        logger.info(f"åˆ†æAPIç›¸ä¼¼æ€? {api_name}")
        
        # æ£€æŸ¥ç¼“å­?
        cache_key = self._get_cache_key(api_name, similarity_threshold, analysis_type)
        if self.config.enable_cache:
            with self.cache_lock:
                if cache_key in self.analysis_cache:
                    logger.info("è¿”å›ç¼“å­˜ç»“æœ")
                    return self.analysis_cache[cache_key]
        
        # æœç´¢æ‰€æœ‰ç›¸å…³API
        all_apis = self._find_all_related_apis(api_name)
        
        if not all_apis:
            logger.warning(f"æœªæ‰¾åˆ°ç›¸å…³API: {api_name}")
            return APIAnalysisResult(
                api_name=api_name,
                total_occurrences=0,
                repos_containing=[],
                similarity_groups=[],
                usage_patterns={}
            )
        
        # æ‰§è¡Œç›¸ä¼¼æ€§åˆ†æ?
        similarity_results = self._analyze_similarities(all_apis, similarity_threshold, analysis_type)
        
        # æ„å»ºç›¸ä¼¼æ€§ç»„
        similarity_groups = self._build_similarity_groups(similarity_results)
        
        # åˆ†æä½¿ç”¨æ¨¡å¼
        usage_patterns = self._analyze_usage_patterns(all_apis)
        
        # æ„å»ºç»“æœ
        result = APIAnalysisResult(
            api_name=api_name,
            total_occurrences=len(all_apis),
            repos_containing=list(set(api.repo_name for api in all_apis)),
            similarity_groups=similarity_groups,
            usage_patterns=usage_patterns
        )
        
        # ç¼“å­˜ç»“æœ
        if self.config.enable_cache:
            with self.cache_lock:
                self.analysis_cache[cache_key] = result
        
        # è®°å½•åˆ†æå†å²
        self._record_analysis(api_name, result)
        
        return result
    
    def find_duplicate_apis(self, 
                          similarity_threshold: float = None,
                          target_repos: Optional[List[str]] = None) -> Dict[str, List[SimilarityResult]]:
        """
        æŸ¥æ‰¾é‡å¤API
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€?
            target_repos: ç›®æ ‡ä»“åº“åˆ—è¡¨
            
        Returns:
            é‡å¤APIåˆ†æç»“æœ
        """
        logger.info("æŸ¥æ‰¾é‡å¤API")
        
        threshold = similarity_threshold or self.config.similarity_threshold
        
        # è·å–æ‰€æœ‰API
        all_apis = self._get_all_apis(target_repos)
        
        if len(all_apis) < 2:
            logger.warning("APIæ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé‡å¤æ£€æµ?)
            return {}
        
        # æ‰§è¡Œé‡å¤æ£€æµ?
        duplicate_groups = defaultdict(list)
        
        if self.config.enable_parallel_analysis:
            duplicate_results = self._parallel_duplicate_detection(all_apis, threshold)
        else:
            duplicate_results = self._sequential_duplicate_detection(all_apis, threshold)
        
        # ç»„ç»‡ç»“æœ
        for result in duplicate_results:
            if result.overall_similarity >= threshold:
                group_key = f"{result.api1.name}_{result.api2.name}"
                duplicate_groups[group_key].append(result)
        
        return dict(duplicate_groups)
    
    def recommend_similar_apis(self, 
                             api_name: str,
                             top_k: int = 10,
                             similarity_threshold: float = None) -> List[Dict[str, Any]]:
        """
        æ¨èç›¸ä¼¼API
        
        Args:
            api_name: APIåç§°
            top_k: è¿”å›çš„æ¨èæ•°é‡?
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€?
            
        Returns:
            ç›¸ä¼¼APIæ¨èåˆ—è¡¨
        """
        logger.info(f"æ¨èç›¸ä¼¼API: {api_name}")
        
        # åˆ†æAPIç›¸ä¼¼æ€?
        analysis_result = self.analyze_api_similarity(api_name, similarity_threshold)
        
        # æ„å»ºæ¨èåˆ—è¡¨
        recommendations = []
        threshold = similarity_threshold or self.config.similarity_threshold
        
        for group in analysis_result.similarity_groups:
            if len(group) > 1:  # åªè€ƒè™‘æœ‰ç›¸ä¼¼APIçš„ç»„
                for api in group:
                    if api.name != api_name:  # æ’é™¤è‡ªèº«
                        # è®¡ç®—ä¸ç›®æ ‡APIçš„ç›¸ä¼¼åº¦
                        similarity_score = self._calculate_api_similarity(
                            self._find_api_by_name(api_name, analysis_result.similarity_groups),
                            api
                        )
                        
                        if similarity_score >= threshold:
                            recommendation = {
                                "api_name": api.name,
                                "similarity_score": similarity_score,
                                "repo_name": api.repo_name,
                                "file_path": api.file_path,
                                "parameters": api.parameters,
                                "return_type": api.return_type,
                                "docstring": api.docstring,
                                "explanation": f"ä¸?{api_name} çš„ç›¸ä¼¼åº¦ä¸?{similarity_score:.3f}"
                            }
                            recommendations.append(recommendation)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æ•°é‡?
        recommendations.sort(key=lambda x: x["similarity_score"], reverse=True)
        return recommendations[:top_k]
    
    def analyze_api_evolution(self, 
                            api_name: str,
                            target_repos: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        åˆ†æAPIæ¼”åŒ–
        
        Args:
            api_name: APIåç§°
            target_repos: ç›®æ ‡ä»“åº“åˆ—è¡¨
            
        Returns:
            APIæ¼”åŒ–åˆ†æç»“æœ
        """
        logger.info(f"åˆ†æAPIæ¼”åŒ–: {api_name}")
        
        # è·å–æ‰€æœ‰ç›¸å…³API
        all_apis = self._find_all_related_apis(api_name, target_repos)
        
        if not all_apis:
            return {"error": f"æœªæ‰¾åˆ°API: {api_name}"}
        
        # åˆ†ææ¼”åŒ–æ¨¡å¼
        evolution_analysis = {
            "api_name": api_name,
            "total_versions": len(all_apis),
            "repos_containing": list(set(api.repo_name for api in all_apis)),
            "parameter_evolution": self._analyze_parameter_evolution(all_apis),
            "return_type_evolution": self._analyze_return_type_evolution(all_apis),
            "docstring_evolution": self._analyze_docstring_evolution(all_apis),
            "usage_frequency": self._analyze_usage_frequency(all_apis)
        }
        
        return evolution_analysis
    
    def _find_all_related_apis(self, 
                              api_name: str,
                              target_repos: Optional[List[str]] = None) -> List[APISignature]:
        """æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³API"""
        all_apis = []
        
        # ä½¿ç”¨å¤šåº“æœç´¢æŸ¥æ‰¾ç›¸å…³API
        search_result = self.multi_repo_search.search_across_repos(
            query=f"API function {api_name}",
            search_type="hybrid",
            target_repos=target_repos
        )
        
        # ä»æœç´¢ç»“æœä¸­æå–APIç­¾å
        for cross_repo_result in search_result.cross_repo_results:
            for result in cross_repo_result.results:
                api_signature = self._extract_api_signature(result, cross_repo_result.repo_name)
                if api_signature:
                    all_apis.append(api_signature)
        
        return all_apis
    
    def _extract_api_signature(self, result, repo_name: str) -> Optional[APISignature]:
        """ä»æœç´¢ç»“æœä¸­æå–APIç­¾å"""
        try:
            content = result.code_embedding.content
            
            # æå–å‡½æ•°å?
            name_match = re.search(r'def\s+(\w+)\s*\(', content)
            if not name_match:
                return None
            
            function_name = name_match.group(1)
            
            # æå–å‚æ•°
            params_match = re.search(r'def\s+\w+\s*\((.*?)\)', content, re.DOTALL)
            parameters = []
            if params_match:
                params_str = params_match.group(1).strip()
                if params_str:
                    parameters = [param.strip().split(':')[0].strip() for param in params_str.split(',')]
            
            # æå–è¿”å›ç±»å‹
            return_type = None
            return_match = re.search(r'->\s*(\w+)', content)
            if return_match:
                return_type = return_match.group(1)
            
            # æå–æ–‡æ¡£å­—ç¬¦ä¸?
            docstring = None
            docstring_match = re.search(r'"""(.*?)"""', content, re.DOTALL)
            if docstring_match:
                docstring = docstring_match.group(1).strip()
            
            return APISignature(
                name=function_name,
                parameters=parameters,
                return_type=return_type,
                docstring=docstring,
                file_path=getattr(result.code_embedding, 'file_path', 'Unknown'),
                line_number=getattr(result.code_embedding, 'line_number', 0),
                repo_name=repo_name
            )
            
        except Exception as e:
            logger.error(f"æå–APIç­¾åå¤±è´¥: {e}")
            return None
    
    def _analyze_similarities(self, 
                            apis: List[APISignature],
                            similarity_threshold: float,
                            analysis_type: str) -> List[SimilarityResult]:
        """åˆ†æAPIç›¸ä¼¼æ€?""
        similarity_results = []
        threshold = similarity_threshold or self.config.similarity_threshold
        
        # æ¯”è¾ƒæ‰€æœ‰APIå¯?
        for i in range(len(apis)):
            for j in range(i + 1, len(apis)):
                similarity_result = self._compare_apis(apis[i], apis[j], analysis_type)
                if similarity_result.overall_similarity >= threshold:
                    similarity_results.append(similarity_result)
        
        return similarity_results
    
    def _compare_apis(self, 
                     api1: APISignature, 
                     api2: APISignature,
                     analysis_type: str) -> SimilarityResult:
        """æ¯”è¾ƒä¸¤ä¸ªAPI"""
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€?
        semantic_similarity = self._calculate_semantic_similarity(api1, api2)
        
        # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€?
        structural_similarity = self._calculate_structural_similarity(api1, api2)
        
        # è®¡ç®—æ•´ä½“ç›¸ä¼¼æ€?
        overall_similarity = (semantic_similarity + structural_similarity) / 2
        
        # ç¡®å®šç›¸ä¼¼æ€§ç±»å?
        similarity_type = self._determine_similarity_type(api1, api2, semantic_similarity, structural_similarity)
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self._generate_similarity_explanation(api1, api2, semantic_similarity, structural_similarity)
        
        return SimilarityResult(
            api1=api1,
            api2=api2,
            semantic_similarity=semantic_similarity,
            structural_similarity=structural_similarity,
            overall_similarity=overall_similarity,
            similarity_type=similarity_type,
            explanation=explanation
        )
    
    def _calculate_semantic_similarity(self, api1: APISignature, api2: APISignature) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€?""
        if not self.vectorizer:
            return 0.0
        
        try:
            # æ„å»ºAPIæè¿°æ–‡æœ¬
            text1 = self._build_api_description(api1)
            text2 = self._build_api_description(api2)
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding1 = self.vectorizer.embed_text(text1)
            embedding2 = self.vectorizer.embed_text(text2)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº?
            return self._calculate_cosine_similarity(embedding1, embedding2)
            
        except Exception as e:
            logger.error(f"è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§å¤±è´? {e}")
            return 0.0
    
    def _calculate_structural_similarity(self, api1: APISignature, api2: APISignature) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼æ€?""
        # å‚æ•°æ•°é‡ç›¸ä¼¼æ€?
        param_count_similarity = self._calculate_parameter_count_similarity(api1.parameters, api2.parameters)
        
        # å‚æ•°ç±»å‹ç›¸ä¼¼æ€?
        param_type_similarity = self._calculate_parameter_type_similarity(api1.parameters, api2.parameters)
        
        # è¿”å›ç±»å‹ç›¸ä¼¼æ€?
        return_type_similarity = self._calculate_return_type_similarity(api1.return_type, api2.return_type)
        
        # åç§°ç›¸ä¼¼æ€?
        name_similarity = self._calculate_name_similarity(api1.name, api2.name)
        
        # åŠ æƒå¹³å‡
        structural_similarity = (
            param_count_similarity * 0.3 +
            param_type_similarity * 0.3 +
            return_type_similarity * 0.2 +
            name_similarity * 0.2
        )
        
        return structural_similarity
    
    def _calculate_parameter_count_similarity(self, params1: List[str], params2: List[str]) -> float:
        """è®¡ç®—å‚æ•°æ•°é‡ç›¸ä¼¼æ€?""
        count1, count2 = len(params1), len(params2)
        if count1 == 0 and count2 == 0:
            return 1.0
        if count1 == 0 or count2 == 0:
            return 0.0
        
        return 1.0 - abs(count1 - count2) / max(count1, count2)
    
    def _calculate_parameter_type_similarity(self, params1: List[str], params2: List[str]) -> float:
        """è®¡ç®—å‚æ•°ç±»å‹ç›¸ä¼¼æ€?""
        if not params1 and not params2:
            return 1.0
        if not params1 or not params2:
            return 0.0
        
        # ç®€å•çš„ç±»å‹åŒ¹é…
        type1 = set(re.findall(r':\s*(\w+)', ' '.join(params1)))
        type2 = set(re.findall(r':\s*(\w+)', ' '.join(params2)))
        
        if not type1 and not type2:
            return 1.0
        if not type1 or not type2:
            return 0.0
        
        intersection = len(type1 & type2)
        union = len(type1 | type2)
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_return_type_similarity(self, return_type1: Optional[str], return_type2: Optional[str]) -> float:
        """è®¡ç®—è¿”å›ç±»å‹ç›¸ä¼¼æ€?""
        if return_type1 is None and return_type2 is None:
            return 1.0
        if return_type1 is None or return_type2 is None:
            return 0.0
        
        return 1.0 if return_type1 == return_type2 else 0.0
    
    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—åç§°ç›¸ä¼¼æ€?""
        # ç®€å•çš„ç¼–è¾‘è·ç¦»ç›¸ä¼¼æ€?
        def levenshtein_distance(s1, s2):
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        distance = levenshtein_distance(name1.lower(), name2.lower())
        max_length = max(len(name1), len(name2))
        
        return 1.0 - (distance / max_length) if max_length > 0 else 0.0
    
    def _calculate_cosine_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº?""
        try:
            dot_product = np.dot(embedding1, embedding2)
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception as e:
            logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´? {e}")
            return 0.0
    
    def _build_api_description(self, api: APISignature) -> str:
        """æ„å»ºAPIæè¿°æ–‡æœ¬"""
        description_parts = [api.name]
        
        if api.parameters:
            description_parts.append(f"parameters: {', '.join(api.parameters)}")
        
        if api.return_type:
            description_parts.append(f"returns: {api.return_type}")
        
        if api.docstring:
            description_parts.append(api.docstring)
        
        return " ".join(description_parts)
    
    def _determine_similarity_type(self, 
                                 api1: APISignature, 
                                 api2: APISignature,
                                 semantic_similarity: float,
                                 structural_similarity: float) -> str:
        """ç¡®å®šç›¸ä¼¼æ€§ç±»å?""
        if api1.name == api2.name:
            return "exact"
        elif semantic_similarity >= self.config.semantic_similarity_threshold:
            return "semantic"
        elif structural_similarity >= self.config.structural_similarity_threshold:
            return "structural"
        else:
            return "hybrid"
    
    def _generate_similarity_explanation(self, 
                                       api1: APISignature, 
                                       api2: APISignature,
                                       semantic_similarity: float,
                                       structural_similarity: float) -> str:
        """ç”Ÿæˆç›¸ä¼¼æ€§è§£é‡?""
        explanations = []
        
        if semantic_similarity >= self.config.semantic_similarity_threshold:
            explanations.append(f"è¯­ä¹‰ç›¸ä¼¼åº¦é«˜ ({semantic_similarity:.3f})")
        
        if structural_similarity >= self.config.structural_similarity_threshold:
            explanations.append(f"ç»“æ„ç›¸ä¼¼åº¦é«˜ ({structural_similarity:.3f})")
        
        if api1.parameters == api2.parameters:
            explanations.append("å‚æ•°å®Œå…¨ç›¸åŒ")
        
        if api1.return_type == api2.return_type:
            explanations.append("è¿”å›ç±»å‹ç›¸åŒ")
        
        return "; ".join(explanations) if explanations else "ç›¸ä¼¼æ€§è¾ƒä½?
    
    def _build_similarity_groups(self, similarity_results: List[SimilarityResult]) -> List[List[APISignature]]:
        """æ„å»ºç›¸ä¼¼æ€§ç»„"""
        # ä½¿ç”¨å¹¶æŸ¥é›†ç®—æ³•æ„å»ºç›¸ä¼¼æ€§ç»„
        api_to_group = {}
        groups = []
        
        for result in similarity_results:
            api1, api2 = result.api1, result.api2
            
            if api1 not in api_to_group and api2 not in api_to_group:
                # åˆ›å»ºæ–°ç»„
                new_group = [api1, api2]
                groups.append(new_group)
                api_to_group[api1] = len(groups) - 1
                api_to_group[api2] = len(groups) - 1
            elif api1 in api_to_group and api2 not in api_to_group:
                # å°†api2æ·»åŠ åˆ°api1çš„ç»„
                group_idx = api_to_group[api1]
                groups[group_idx].append(api2)
                api_to_group[api2] = group_idx
            elif api2 in api_to_group and api1 not in api_to_group:
                # å°†api1æ·»åŠ åˆ°api2çš„ç»„
                group_idx = api_to_group[api2]
                groups[group_idx].append(api1)
                api_to_group[api1] = group_idx
            elif api1 in api_to_group and api2 in api_to_group and api_to_group[api1] != api_to_group[api2]:
                # åˆå¹¶ä¸¤ä¸ªç»?
                group1_idx = api_to_group[api1]
                group2_idx = api_to_group[api2]
                
                # å°†group2çš„æ‰€æœ‰APIæ·»åŠ åˆ°group1
                groups[group1_idx].extend(groups[group2_idx])
                
                # æ›´æ–°æ‰€æœ‰APIçš„ç»„ç´¢å¼•
                for api in groups[group2_idx]:
                    api_to_group[api] = group1_idx
                
                # åˆ é™¤group2
                groups.pop(group2_idx)
                
                # æ›´æ–°æ‰€æœ‰ç»„ç´¢å¼•
                for i, group in enumerate(groups):
                    for api in group:
                        api_to_group[api] = i
        
        return groups
    
    def _analyze_usage_patterns(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æä½¿ç”¨æ¨¡å¼"""
        patterns = {
            "total_apis": len(apis),
            "repo_distribution": Counter(api.repo_name for api in apis),
            "parameter_patterns": self._analyze_parameter_patterns(apis),
            "return_type_patterns": self._analyze_return_type_patterns(apis),
            "naming_patterns": self._analyze_naming_patterns(apis)
        }
        
        return patterns
    
    def _analyze_parameter_patterns(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æå‚æ•°æ¨¡å¼"""
        param_counts = [len(api.parameters) for api in apis]
        
        return {
            "average_parameter_count": np.mean(param_counts) if param_counts else 0,
            "max_parameter_count": max(param_counts) if param_counts else 0,
            "min_parameter_count": min(param_counts) if param_counts else 0,
            "parameter_count_distribution": Counter(param_counts)
        }
    
    def _analyze_return_type_patterns(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æè¿”å›ç±»å‹æ¨¡å¼"""
        return_types = [api.return_type for api in apis if api.return_type]
        
        return {
            "total_with_return_type": len(return_types),
            "return_type_distribution": Counter(return_types),
            "most_common_return_type": Counter(return_types).most_common(1)[0] if return_types else None
        }
    
    def _analyze_naming_patterns(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æå‘½åæ¨¡å¼"""
        names = [api.name for api in apis]
        
        # åˆ†æå‘½åå‰ç¼€
        prefixes = [name.split('_')[0] for name in names if '_' in name]
        
        return {
            "total_apis": len(names),
            "unique_names": len(set(names)),
            "naming_prefixes": Counter(prefixes),
            "most_common_prefix": Counter(prefixes).most_common(1)[0] if prefixes else None
        }
    
    def _get_all_apis(self, target_repos: Optional[List[str]] = None) -> List[APISignature]:
        """è·å–æ‰€æœ‰API"""
        all_apis = []
        
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„å¤šåº“æœç´¢å™¨æ¥å£è°ƒæ•?
        # å‡è®¾æœ‰ä¸€ä¸ªæ–¹æ³•å¯ä»¥è·å–æ‰€æœ‰API
        repos_to_search = target_repos or list(self.multi_repo_search.repo_searchers.keys())
        
        for repo_name in repos_to_search:
            if repo_name in self.multi_repo_search.repo_searchers:
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æœç´¢å™¨æ¥å£è°ƒæ•?
                # å‡è®¾å¯ä»¥æœç´¢æ‰€æœ‰å‡½æ•?
                try:
                    searcher = self.multi_repo_search.repo_searchers[repo_name]
                    # æœç´¢æ‰€æœ‰å‡½æ•?
                    results = searcher.search_by_natural_language(
                        query="function definition",
                        limit=self.config.max_apis_per_repo,
                        search_type="keyword"
                    )
                    
                    for result in results:
                        api_signature = self._extract_api_signature(result, repo_name)
                        if api_signature:
                            all_apis.append(api_signature)
                            
                except Exception as e:
                    logger.error(f"è·å–ä»“åº“ {repo_name} çš„APIå¤±è´¥: {e}")
        
        return all_apis
    
    def _parallel_duplicate_detection(self, apis: List[APISignature], threshold: float) -> List[SimilarityResult]:
        """å¹¶è¡Œé‡å¤æ£€æµ?""
        duplicate_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ¯”è¾ƒä»»åŠ¡
            futures = []
            for i in range(len(apis)):
                for j in range(i + 1, len(apis)):
                    future = executor.submit(self._compare_apis, apis[i], apis[j], "comprehensive")
                    futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    if result.overall_similarity >= threshold:
                        duplicate_results.append(result)
                except Exception as e:
                    logger.error(f"å¹¶è¡Œé‡å¤æ£€æµ‹å¤±è´? {e}")
        
        return duplicate_results
    
    def _sequential_duplicate_detection(self, apis: List[APISignature], threshold: float) -> List[SimilarityResult]:
        """é¡ºåºé‡å¤æ£€æµ?""
        duplicate_results = []
        
        for i in range(len(apis)):
            for j in range(i + 1, len(apis)):
                try:
                    result = self._compare_apis(apis[i], apis[j], "comprehensive")
                    if result.overall_similarity >= threshold:
                        duplicate_results.append(result)
                except Exception as e:
                    logger.error(f"æ¯”è¾ƒAPIå¤±è´¥: {e}")
        
        return duplicate_results
    
    def _analyze_parameter_evolution(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æå‚æ•°æ¼”åŒ–"""
        param_evolution = {
            "parameter_count_evolution": [len(api.parameters) for api in apis],
            "parameter_type_evolution": [api.parameters for api in apis],
            "evolution_patterns": []
        }
        
        # åˆ†ææ¼”åŒ–æ¨¡å¼
        if len(apis) > 1:
            # æŒ‰ä»“åº“åˆ†ç»„åˆ†æ?
            repo_groups = defaultdict(list)
            for api in apis:
                repo_groups[api.repo_name].append(api)
            
            for repo_name, repo_apis in repo_groups.items():
                if len(repo_apis) > 1:
                    # åˆ†æåŒä¸€ä»“åº“å†…çš„æ¼”åŒ–
                    param_counts = [len(api.parameters) for api in repo_apis]
                    if len(set(param_counts)) > 1:
                        param_evolution["evolution_patterns"].append({
                            "repo": repo_name,
                            "parameter_count_changes": param_counts,
                            "has_evolution": True
                        })
        
        return param_evolution
    
    def _analyze_return_type_evolution(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æè¿”å›ç±»å‹æ¼”åŒ–"""
        return_evolution = {
            "return_type_evolution": [api.return_type for api in apis],
            "evolution_patterns": []
        }
        
        # åˆ†ææ¼”åŒ–æ¨¡å¼
        if len(apis) > 1:
            return_types = [api.return_type for api in apis if api.return_type]
            if len(set(return_types)) > 1:
                return_evolution["evolution_patterns"].append({
                    "has_return_type_evolution": True,
                    "return_types": return_types
                })
        
        return return_evolution
    
    def _analyze_docstring_evolution(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£å­—ç¬¦ä¸²æ¼”åŒ?""
        docstring_evolution = {
            "docstring_evolution": [api.docstring for api in apis if api.docstring],
            "evolution_patterns": []
        }
        
        # åˆ†ææ¼”åŒ–æ¨¡å¼
        if len(apis) > 1:
            docstrings = [api.docstring for api in apis if api.docstring]
            if len(docstrings) > 1:
                docstring_evolution["evolution_patterns"].append({
                    "has_docstring_evolution": True,
                    "docstring_count": len(docstrings)
                })
        
        return docstring_evolution
    
    def _analyze_usage_frequency(self, apis: List[APISignature]) -> Dict[str, Any]:
        """åˆ†æä½¿ç”¨é¢‘ç‡"""
        usage_frequency = {
            "repo_usage_frequency": Counter(api.repo_name for api in apis),
            "total_usage_count": len(apis),
            "most_used_repo": Counter(api.repo_name for api in apis).most_common(1)[0] if apis else None
        }
        
        return usage_frequency
    
    def _find_api_by_name(self, api_name: str, similarity_groups: List[List[APISignature]]) -> Optional[APISignature]:
        """æ ¹æ®åç§°æŸ¥æ‰¾API"""
        for group in similarity_groups:
            for api in group:
                if api.name == api_name:
                    return api
        return None
    
    def _calculate_api_similarity(self, api1: Optional[APISignature], api2: APISignature) -> float:
        """è®¡ç®—APIç›¸ä¼¼åº?""
        if not api1:
            return 0.0
        
        result = self._compare_apis(api1, api2, "comprehensive")
        return result.overall_similarity
    
    def _get_cache_key(self, api_name: str, similarity_threshold: float, analysis_type: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”?""
        import hashlib
        key_data = f"{api_name}_{similarity_threshold}_{analysis_type}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _record_analysis(self, api_name: str, result: APIAnalysisResult):
        """è®°å½•åˆ†æå†å²"""
        analysis_record = {
            "timestamp": json.dumps({"timestamp": "now"}),
            "api_name": api_name,
            "total_occurrences": result.total_occurrences,
            "repos_containing": len(result.repos_containing),
            "similarity_groups": len(result.similarity_groups)
        }
        self.analysis_history.append(analysis_record)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.analysis_history) > 100:
            self.analysis_history = self.analysis_history[-100:]
    
    def get_analysis_history(self) -> List[Dict[str, Any]]:
        """è·å–åˆ†æå†å²"""
        return self.analysis_history.copy()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.cache_lock:
            self.analysis_cache.clear()
        logger.info("APIç›¸ä¼¼æ€§åˆ†æç¼“å­˜å·²æ¸…ç©º")

def create_api_similarity_analyzer(multi_repo_search: MultiRepoSearch,
                                 config: APISimilarityConfig = None) -> APISimilarityAnalyzer:
    """åˆ›å»ºAPIç›¸ä¼¼æ€§åˆ†æå™¨å®ä¾‹"""
    return APISimilarityAnalyzer(multi_repo_search, config)
