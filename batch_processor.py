#!/usr/bin/env python3

import os
import sys
import json
import time
import subprocess
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import logging
import threading
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_processor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™?""
    
    def __init__(self, total_repos: int):
        self.total_repos = total_repos
        self.current_repo = 0
        self.start_time = time.time()
        self.repo_times = []
        self.stage_times = defaultdict(list)
        self.lock = threading.Lock()
    
    def start_repo(self, repo_name: str):
        """å¼€å§‹å¤„ç†æ–°ä»“åº“"""
        with self.lock:
            self.current_repo += 1
            self.repo_start_time = time.time()
            print(f"\n{'='*80}")
            print(f"[{self.current_repo}/{self.total_repos}] å¼€å§‹å¤„ç†ä»“åº? {repo_name}")
            print(f"{'='*80}")
    
    def end_repo(self, repo_name: str, success: bool = True):
        """ç»“æŸå½“å‰ä»“åº“å¤„ç†"""
        with self.lock:
            repo_time = time.time() - self.repo_start_time
            self.repo_times.append(repo_time)
            
            status = "æˆåŠŸ" if success else "å¤±è´¥"
            print(f"\n{status} å®Œæˆä»“åº“ {repo_name} (è€—æ—¶: {self._format_time(repo_time)})")
            
            # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
            avg_time = sum(self.repo_times) / len(self.repo_times)
            remaining_repos = self.total_repos - self.current_repo
            estimated_remaining = avg_time * remaining_repos
            
            print(f"æ€»ä½“è¿›åº¦: {self.current_repo}/{self.total_repos} ({self.current_repo/self.total_repos*100:.1f}%)")
            print(f"å¹³å‡è€—æ—¶: {self._format_time(avg_time)}")
            if remaining_repos > 0:
                print(f"é¢„è®¡å‰©ä½™æ—¶é—´: {self._format_time(estimated_remaining)}")
    
    def start_stage(self, stage_name: str):
        """å¼€å§‹æ–°é˜¶æ®µ"""
        with self.lock:
            self.stage_start_time = time.time()
            print(f"\nå¼€å§‹é˜¶æ®? {stage_name}")
    
    def end_stage(self, stage_name: str, details: str = ""):
        """ç»“æŸå½“å‰é˜¶æ®µ"""
        with self.lock:
            stage_time = time.time() - self.stage_start_time
            self.stage_times[stage_name].append(stage_time)
            print(f"å®Œæˆé˜¶æ®µ: {stage_name} (è€—æ—¶: {self._format_time(stage_time)})")
            if details:
                print(f"   {details}")
    
    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤?""
        if seconds < 60:
            return f"{seconds:.1f}ç§?
        elif seconds < 3600:
            return f"{seconds/60:.1f}åˆ†é’Ÿ"
        else:
            return f"{seconds/3600:.1f}å°æ—¶"
    
    def get_final_stats(self) -> Dict[str, Any]:
        """è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ?""
        total_time = time.time() - self.start_time
        return {
            "total_repos": self.total_repos,
            "processed_repos": len(self.repo_times),
            "total_time": total_time,
            "average_repo_time": sum(self.repo_times) / len(self.repo_times) if self.repo_times else 0,
            "stage_times": dict(self.stage_times)
        }


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™?""
    
    def __init__(self, 
                 txt_file: str,
                 neo4j_uri: str = "bolt://localhost:7687",
                 neo4j_user: str = "neo4j", 
                 neo4j_password: str = "90879449Drq",
                 neo4j_database: Optional[str] = None,
                 clean_db: bool = False,
                 filter_large_functions: bool = False,
                 concurrent: int = 4,
                 batch_size: int = 32,
                 use_cache: bool = True,
                 model_name: str = "all-MiniLM-L6-v2",
                 model_type: str = "sentence_transformers"):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            txt_file: åŒ…å«GitHubåº“é“¾æ¥çš„txtæ–‡ä»¶è·¯å¾„
            neo4j_uri: Neo4jæ•°æ®åº“URI
            neo4j_user: Neo4jç”¨æˆ·å?            neo4j_password: Neo4jå¯†ç 
            neo4j_database: Neo4jæ•°æ®åº“å
            clean_db: æ˜¯å¦æ¸…ç©ºæ•°æ®åº?            filter_large_functions: æ˜¯å¦è¿‡æ»¤å¤§å‡½æ•?            concurrent: å¹¶å‘æ•?            batch_size: æ‰¹å¤„ç†å¤§å°?            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            model_name: åµŒå…¥æ¨¡å‹åç§°
            model_type: åµŒå…¥æ¨¡å‹ç±»å‹
        """
        self.txt_file = txt_file
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.neo4j_database = neo4j_database
        self.clean_db = clean_db
        self.filter_large_functions = filter_large_functions
        self.concurrent = concurrent
        self.batch_size = batch_size
        self.use_cache = use_cache
        self.model_name = model_name
        self.model_type = model_type
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_repos': 0,
            'successful_repos': 0,
            'failed_repos': 0,
            'start_time': None,
            'end_time': None,
            'repo_results': []
        }
    
    def read_repo_urls(self) -> List[str]:
        """è¯»å–GitHubåº“é“¾æ¥å¹¶æ·»åŠ .gitåç¼€"""
        try:
            with open(self.txt_file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
            
            # ä¸ºæ¯ä¸ªé“¾æ¥æ·»åŠ?gitåç¼€ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼?            processed_urls = []
            for url in urls:
                if url.endswith('.git'):
                    processed_urls.append(url)
                else:
                    processed_urls.append(url + '.git')
            
            logger.info(f"ä»?{self.txt_file} è¯»å–åˆ?{len(processed_urls)} ä¸ªGitHubåº“é“¾æ?)
            logger.debug(f"å¤„ç†åçš„é“¾æ¥ç¤ºä¾‹: {processed_urls[:3] if processed_urls else []}")
            return processed_urls
        except FileNotFoundError:
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ? {self.txt_file}")
            raise
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def extract_repo_name(self, url: str) -> str:
        """ä»GitHub URLæå–ä»“åº“åï¼Œä¸extract.mainä¿æŒä¸€è‡?""
        from urllib.parse import urlparse
        from pathlib import Path
        
        # ä½¿ç”¨ä¸extract.mainç›¸åŒçš„é€»è¾‘
        parsed = urlparse(url)
        repo_name = Path(parsed.path).stem  # åªå–è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼Œå»æ?git
        
        return repo_name
    
    def run_extract(self, repo_url: str) -> Optional[str]:
        """è¿è¡Œextractæµç¨‹"""
        repo_name = self.extract_repo_name(repo_url)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆextractæ­¥éª¤
        output_file = os.path.join("output/extract_output", f"{repo_name}_api_extraction.json")
        if os.path.exists(output_file):
            logger.info(f"extractæ­¥éª¤å·²å®Œæˆï¼Œè·³è¿‡: {output_file}")
            return output_file
        
        logger.info(f"å¼€å§‹extractæµç¨‹: {repo_name}")
        
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [
                sys.executable, "-m", "extract.main", 
                repo_url
            ]
            
            if self.filter_large_functions:
                cmd.append("--filter-large-functions")
            
            # æ‰§è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº?            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            print(f"\næ­£åœ¨æå–APIä¿¡æ¯: {repo_name}")
            
            # ä½¿ç”¨å®æ—¶è¾“å‡ºæ˜¾ç¤ºè¿›åº¦
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=os.getcwd(),
                env=os.environ.copy(),
                bufsize=1,
                universal_newlines=True
            )
            
            # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # æ¸…ç†è¾“å‡ºå¹¶æ˜¾ç¤ºè¿›åº¦ä¿¡æ?                    line = output.strip()
                    if line and not line.startswith('DEBUG'):
                        print(f"  [æ–‡ä»¶] {line}")
                        # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒ?                        sys.stdout.flush()
            
            result = subprocess.CompletedProcess(
                args=cmd,
                returncode=process.returncode,
                stdout="",  # å·²ç»å®æ—¶æ˜¾ç¤ºäº?                stderr=""
            )
            
            # è®°å½•è¯¦ç»†çš„æ‰§è¡Œç»“æ?            logger.info(f"å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç : {result.returncode}")
            if result.stdout:
                logger.debug(f"æ ‡å‡†è¾“å‡º: {result.stdout}")
            if result.stderr:
                logger.debug(f"æ ‡å‡†é”™è¯¯: {result.stderr}")
            
            if result.returncode == 0:
                output_file = os.path.join("output/extract_output", f"{repo_name}_api_extraction.json")
                if os.path.exists(output_file):
                    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†å‡½æ•°
                    try:
                        with open(output_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            total_functions = data.get('total_functions', 0)
                            if total_functions == 0:
                                logger.warning(f"extractå®Œæˆä½†æœªæ‰¾åˆ°ä»»ä½•å‡½æ•°: {repo_name} (å¯èƒ½æ˜¯éæ”¯æŒè¯­è¨€çš„åº“æˆ–ç©ºåº?")
                                return "NO_FUNCTIONS_FOUND"  # ç‰¹æ®Šæ ‡è®°è¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°å‡½æ•°
                            else:
                                logger.info(f"extractå®Œæˆ: {output_file} (æ‰¾åˆ° {total_functions} ä¸ªå‡½æ•?")
                                return output_file
                    except Exception as e:
                        logger.error(f"æ— æ³•è§£æextractè¾“å‡ºæ–‡ä»¶: {output_file}, é”™è¯¯: {str(e)}")
                        return None
                else:
                    logger.error(f"extractè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ? {output_file}")
                    return None
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯KeyErrorç›¸å…³çš„é”™è¯?                if "KeyError" in result.stderr and "direct_call_weight" in result.stderr:
                    logger.warning(f"extracté‡åˆ°KeyErrorï¼Œä½†å¯èƒ½å·²ç”Ÿæˆéƒ¨åˆ†ç»“æ? {repo_name}")
                    # æ£€æŸ¥æ˜¯å¦ä»ç„¶ç”Ÿæˆäº†è¾“å‡ºæ–‡ä»¶
                    output_file = os.path.join("output/extract_output", f"{repo_name}_api_extraction.json")
                    if os.path.exists(output_file):
                        logger.info(f"å°½ç®¡æœ‰KeyErrorï¼Œä½†æ‰¾åˆ°äº†è¾“å‡ºæ–‡ä»? {output_file}")
                        return output_file
                    else:
                        logger.error(f"KeyErrorå¯¼è‡´extractå®Œå…¨å¤±è´¥: {repo_name}")
                        return None
                else:
                    logger.error(f"extractå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                    logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                    if result.stdout:
                        logger.error(f"æ ‡å‡†è¾“å‡º: {result.stdout}")
                    return None
                
        except subprocess.TimeoutExpired:
            logger.error(f"extractè¶…æ—¶: {repo_name}")
            return None
        except Exception as e:
            logger.error(f"extractå¼‚å¸¸: {repo_name}, é”™è¯¯: {str(e)}")
            return None
    
    def run_description(self, extraction_file: str) -> Optional[str]:
        """è¿è¡Œdescriptionæµç¨‹"""
        # ä»å®Œæ•´è·¯å¾„ä¸­æå–repo_name
        filename = os.path.basename(extraction_file)
        repo_name = filename.replace("_api_extraction.json", "")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆdescriptionæ­¥éª¤
        complete_file = os.path.join("output/description_output", "complete", f"{repo_name}_complete_for_neo4j.json")
        if os.path.exists(complete_file):
            logger.info(f"descriptionæ­¥éª¤å·²å®Œæˆï¼Œè·³è¿‡: {complete_file}")
            return complete_file
        
        logger.info(f"å¼€å§‹descriptionæµç¨‹: {repo_name}")
        
        try:
            # æ„å»ºå‘½ä»¤ï¼Œæ·»åŠ?-skip-complexityå‚æ•°ä»¥ä½¿ç”¨é»˜è®¤moderateå¤æ‚åº?é‡è¦åº?            cmd = [
                sys.executable, "-m", "description.main",
                extraction_file,
                "--skip-complexity"
            ]
            
            # æ‰§è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº?            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            print(f"\næ­£åœ¨ç”Ÿæˆå‡½æ•°æè¿°: {repo_name}")
            
            # ä½¿ç”¨å®æ—¶è¾“å‡ºæ˜¾ç¤ºè¿›åº¦
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=os.getcwd(),
                env=os.environ.copy(),
                bufsize=1,
                universal_newlines=True
            )
            
            # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # æ¸…ç†è¾“å‡ºå¹¶æ˜¾ç¤ºè¿›åº¦ä¿¡æ?                    line = output.strip()
                    if line and not line.startswith('DEBUG'):
                        print(f"  [AI] {line}")
                        # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒ?                        sys.stdout.flush()
            
            result = subprocess.CompletedProcess(
                args=cmd,
                returncode=process.returncode,
                stdout="",  # å·²ç»å®æ—¶æ˜¾ç¤ºäº?                stderr=""
            )
            
            # è®°å½•è¯¦ç»†çš„æ‰§è¡Œç»“æ?            logger.info(f"å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç : {result.returncode}")
            if result.stdout:
                logger.debug(f"æ ‡å‡†è¾“å‡º: {result.stdout}")
            if result.stderr:
                logger.debug(f"æ ‡å‡†é”™è¯¯: {result.stderr}")
            
            if result.returncode == 0:
                complete_file = os.path.join("output/description_output", "complete", f"{repo_name}_complete_for_neo4j.json")
                if os.path.exists(complete_file):
                    logger.info(f"descriptionå®Œæˆ: {complete_file}")
                    return complete_file
                else:
                    logger.error(f"descriptionè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ? {complete_file}")
                    return None
            else:
                logger.error(f"descriptionå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                if result.stdout:
                    logger.error(f"æ ‡å‡†è¾“å‡º: {result.stdout}")
                return None
                
        except subprocess.TimeoutExpired:
            logger.error(f"descriptionè¶…æ—¶: {repo_name}")
            return None
        except Exception as e:
            logger.error(f"descriptionå¼‚å¸¸: {e}")
            return None
    
    def run_vector_embedding(self, complete_file: str) -> bool:
        """è¿è¡Œvector_embeddingæµç¨‹"""
        # ä»å®Œæ•´è·¯å¾„ä¸­æå–repo_name
        filename = os.path.basename(complete_file)
        repo_name = filename.replace("_complete_for_neo4j.json", "")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆvector_embeddingæ­¥éª¤
        index_file = os.path.join("output/vector_embedding_output", "embeddings", f"{repo_name}_index_embeddings.json")
        if os.path.exists(index_file):
            logger.info(f"vector_embeddingæ­¥éª¤å·²å®Œæˆï¼Œè·³è¿‡: {index_file}")
            return True
        
        logger.info(f"å¼€å§‹vector_embeddingæµç¨‹: {repo_name}")
        print(f"\næ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥: {repo_name}")
        
        try:
            # æ„å»ºå‘½ä»¤
            # æ­£ç¡®å¤„ç†æ–‡ä»¶è·¯å¾„å’Œå˜é‡ï¼Œé¿å…è½¬ä¹‰é—®é¢˜
            import json as json_module
            complete_file_escaped = json_module.dumps(complete_file)
            model_name_escaped = json_module.dumps(self.model_name)
            model_type_escaped = json_module.dumps(self.model_type)
            # å¸ƒå°”å€¼éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºjson.dumpsä¼šè¾“å‡ºå°å†™çš„true/false
            use_cache_escaped = str(self.use_cache)
            repo_name_escaped = json_module.dumps(repo_name)
            
            cmd = [
                sys.executable, "-c",
                f"""
import sys
sys.path.append('.')
from semantic_search.vector_embedding import CodeEmbeddingManager, EmbeddingConfig
import json

print("  åŠ è½½æ•°æ®...")
# åŠ è½½æ•°æ®
complete_file = {complete_file_escaped}
with open(complete_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

print("  åˆ›å»ºé…ç½®...")
# åˆ›å»ºé…ç½®
config = EmbeddingConfig(
    model_name={model_name_escaped},
    model_type={model_type_escaped},
    cache_dir='./embedding_cache',
    use_cache={use_cache_escaped}
)

# åˆ›å»ºç®¡ç†å™?manager = CodeEmbeddingManager(config)

# æå–å‡½æ•°æ•°æ®
functions = data.get('functions', [])
if not functions:
    print("æ²¡æœ‰æ‰¾åˆ°å‡½æ•°æ•°æ®")
    sys.exit(1)

# å‡†å¤‡åµŒå…¥æ•°æ®
texts = []
metadata_list = []

for func in functions:
    # æå–å‡½æ•°ä»£ç 
    source_code = func.get('basic_info', {{}}).get('source_code', '')
    if not source_code:
        continue
    
    # æå–å…ƒæ•°æ?    metadata = {{
        'name': func.get('basic_info', {{}}).get('function_name', ''),
        'filepath': func.get('context', {{}}).get('file_path', ''),
        'parent_class_name': func.get('context', {{}}).get('parent_class', ''),
        'parameters_count': func.get('complexity', {{}}).get('semantic_complexity', {{}}).get('parameters', 0),
        'return_type': func.get('basic_info', {{}}).get('return_type', ''),
        'docstring_description': func.get('description_info', {{}}).get('docstring', {{}}).get('description', ''),
        'complexity_score': func.get('complexity', {{}}).get('semantic_complexity', {{}}).get('complexity_score', 0),
        'function_type': 'test' if func.get('basic_info', {{}}).get('function_name', '').startswith('test_') else 'regular'
    }}
    
    texts.append(source_code)
    metadata_list.append(metadata)

if not texts:
    print("æ²¡æœ‰æœ‰æ•ˆçš„å‡½æ•°ä»£ç ?)
    sys.exit(1)

print(f"å¼€å§‹ç”ŸæˆåµŒå…¥ï¼Œå…?{{len(texts)}} ä¸ªå‡½æ•?)

# ç”ŸæˆåµŒå…¥
embeddings = manager.vectorizer.embed_texts(texts, metadata_list)
manager.add_embeddings(embeddings)

# åˆ›å»ºè¾“å‡ºç›®å½•å’Œå­ç›®å½•
import os
output_dir = 'output/vector_embedding_output'
embeddings_dir = os.path.join(output_dir, 'embeddings')
vectorizers_dir = os.path.join(output_dir, 'vectorizers')

os.makedirs(embeddings_dir, exist_ok=True)
os.makedirs(vectorizers_dir, exist_ok=True)

# ä¿å­˜ç´¢å¼•åˆ°embeddingså­æ–‡ä»¶å¤¹
repo_name = {repo_name_escaped}
index_file = os.path.join(embeddings_dir, f'{{repo_name}}_index_embeddings.json')
manager.save_index(index_file)

# ä¿å­˜å‘é‡åŒ–å™¨åˆ°vectorizerså­æ–‡ä»¶å¤¹ï¼ˆå¦‚æœæ˜¯TF-IDFï¼?if config.model_type == 'tfidf':
    vectorizer_file = os.path.join(vectorizers_dir, f'{{repo_name}}_index_vectorizer.pkl')
    manager.vectorizer.save_vectorizer(vectorizer_file)

# æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
stats = manager.get_stats()
print(f"åµŒå…¥å®Œæˆ: {{stats}}")

manager.close()
print("vector_embeddingæµç¨‹å®Œæˆ")
"""
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
            )
            
            if result.returncode == 0:
                index_file = os.path.join("output/vector_embedding_output", "embeddings", f"{repo_name}_index_embeddings.json")
                if os.path.exists(index_file):
                    logger.info(f"vector_embeddingå®Œæˆ: {index_file}")
                    return True
                else:
                    logger.error(f"vector_embeddingè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ? {index_file}")
                    return False
            else:
                logger.error(f"vector_embeddingå¤±è´¥: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error(f"vector_embeddingè¶…æ—¶: {repo_name}")
            return False
        except Exception as e:
            logger.error(f"vector_embeddingå¼‚å¸¸: {e}")
            return False
    
    def run_ingest(self, complete_file: str) -> bool:
        """è¿è¡Œingestæµç¨‹"""
        # ä»å®Œæ•´è·¯å¾„ä¸­æå–repo_name
        filename = os.path.basename(complete_file)
        repo_name = filename.replace("_complete_for_neo4j.json", "")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆingestæ­¥éª¤ï¼ˆé€šè¿‡æ£€æŸ¥æ ‡è®°æ–‡ä»¶ï¼‰
        ingest_marker_file = os.path.join("output/ingest_output", f"{repo_name}_ingest_complete.txt")
        if os.path.exists(ingest_marker_file):
            logger.info(f"ingestæ­¥éª¤å·²å®Œæˆï¼Œè·³è¿‡: {repo_name}")
            return True
        
        logger.info(f"å¼€å§‹ingestæµç¨‹: {repo_name}")
        print(f"\næ­£åœ¨å¯¼å…¥Neo4jæ•°æ®åº? {repo_name}")
        
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [
                sys.executable, "ingest.py",
                complete_file,
                "--uri", self.neo4j_uri,
                "--user", self.neo4j_user,
                "--password", self.neo4j_password
            ]
            
            if self.neo4j_database:
                cmd.extend(["--database", self.neo4j_database])
            
            if self.clean_db:
                cmd.append("--clean-db")
            
            # æ‰§è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº?            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # ä½¿ç”¨å®æ—¶è¾“å‡ºæ˜¾ç¤ºè¿›åº¦
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=os.getcwd(),
                env=os.environ.copy(),
                bufsize=1,
                universal_newlines=True
            )
            
            # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # æ¸…ç†è¾“å‡ºå¹¶æ˜¾ç¤ºè¿›åº¦ä¿¡æ?                    line = output.strip()
                    if line and not line.startswith('DEBUG'):
                        print(f"  [DB] {line}")
                        # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒ?                        sys.stdout.flush()
            
            result = subprocess.CompletedProcess(
                args=cmd,
                returncode=process.returncode,
                stdout="",  # å·²ç»å®æ—¶æ˜¾ç¤ºäº?                stderr=""
            )
            
            if result.returncode == 0:
                logger.info(f"ingestå®Œæˆ: {repo_name}")
                # åˆ›å»ºæ ‡è®°æ–‡ä»¶è¡¨ç¤ºingestå·²å®Œæˆ?                os.makedirs("output/ingest_output", exist_ok=True)
                with open(ingest_marker_file, 'w', encoding='utf-8') as f:
                    f.write(f"ingest completed at {datetime.now().isoformat()}\n")
                return True
            else:
                logger.error(f"ingestå¤±è´¥: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error(f"ingestè¶…æ—¶: {repo_name}")
            return False
        except Exception as e:
            logger.error(f"ingestå¼‚å¸¸: {e}")
            return False
    
    def process_single_repo(self, repo_url: str, progress_tracker: ProgressTracker = None) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªä»“åº“"""
        repo_name = self.extract_repo_name(repo_url)
        result = {
            'repo_url': repo_url,
            'repo_name': repo_name,
            'success': False,
            'steps': {
                'extract': False,
                'description': False,
                'vector_embedding': False,
                'ingest': False
            },
            'error': None,
            'start_time': datetime.now().isoformat(),
            'end_time': None,
            'step_details': {}
        }
        
        try:
            if progress_tracker:
                progress_tracker.start_repo(repo_name)
            
            logger.info(f"å¼€å§‹å¤„ç†ä»“åº? {repo_name} ({repo_url})")
            
            # Step 1: Extract
            if progress_tracker:
                progress_tracker.start_stage("APIæå– (Extract)")
            
            extraction_file = self.run_extract(repo_url)
            if extraction_file:
                result['steps']['extract'] = True
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ²¡æœ‰æ‰¾åˆ°å‡½æ•°çš„æƒ…å†?                if extraction_file == "NO_FUNCTIONS_FOUND":
                    logger.info(f"è·³è¿‡åç»­æ­¥éª¤: {repo_name} (æœªæ‰¾åˆ°ä»»ä½•å‡½æ•?")
                    result['step_details']['extract'] = {
                        'function_count': 0,
                        'file_count': 0,
                        'skip_reason': 'No functions found (possibly non-supported language repository or empty repository)'
                    }
                    result['steps']['description'] = True  # æ ‡è®°ä¸ºè·³è¿?                    result['steps']['vector_embedding'] = True  # æ ‡è®°ä¸ºè·³è¿?                    result['steps']['ingest'] = True  # æ ‡è®°ä¸ºè·³è¿?                    result['step_details']['description'] = {'skip_reason': 'No functions to process'}
                    result['step_details']['vector_embedding'] = {'skip_reason': 'No functions to process'}
                    result['step_details']['ingest'] = {'skip_reason': 'No functions to process'}
                    
                    if progress_tracker:
                        progress_tracker.end_stage("APIæå– (Extract)", "æœªæ‰¾åˆ°ä»»ä½•å‡½æ•°ï¼Œè·³è¿‡åç»­æ­¥éª¤")
                        progress_tracker.end_stage("å‡½æ•°æè¿°ç”Ÿæˆ (Description)", "è·³è¿‡ (æ— å‡½æ•?")
                        progress_tracker.end_stage("å‘é‡åµŒå…¥ (Vector Embedding)", "è·³è¿‡ (æ— å‡½æ•?")
                        progress_tracker.end_stage("Neo4jå¯¼å…¥ (Ingest)", "è·³è¿‡ (æ— å‡½æ•?")
                        progress_tracker.end_repo(repo_name, True)
                    
                    result['end_time'] = datetime.now().isoformat()
                    return result
                
                # è·å–extractç»“æœçš„ç»Ÿè®¡ä¿¡æ?                try:
                    with open(extraction_file, 'r', encoding='utf-8') as f:
                        extract_data = json.load(f)
                        function_count = len(extract_data.get('functions', []))
                        result['step_details']['extract'] = {
                            'function_count': function_count,
                            'file_count': extract_data.get('total_files', 0)
                        }
                        extract_details = f"æå–äº?{function_count} ä¸ªå‡½æ•°ï¼Œ{extract_data.get('total_files', 0)} ä¸ªæ–‡ä»?
                except:
                    extract_details = "æå–å®Œæˆ"
                
                if progress_tracker:
                    progress_tracker.end_stage("APIæå– (Extract)", extract_details)
            else:
                result['error'] = "extractæ­¥éª¤å¤±è´¥"
                if progress_tracker:
                    progress_tracker.end_repo(repo_name, False)
                return result
            
            # Step 2: Description
            if progress_tracker:
                progress_tracker.start_stage("å‡½æ•°æè¿°ç”Ÿæˆ (Description)")
            
            complete_file = self.run_description(extraction_file)
            if complete_file:
                result['steps']['description'] = True
                # è·å–descriptionç»“æœçš„ç»Ÿè®¡ä¿¡æ?                try:
                    with open(complete_file, 'r', encoding='utf-8') as f:
                        desc_data = json.load(f)
                        desc_count = len(desc_data.get('functions', []))
                        result['step_details']['description'] = {
                            'function_count': desc_count
                        }
                        desc_details = f"ç”Ÿæˆäº?{desc_count} ä¸ªå‡½æ•°çš„æè¿°"
                except:
                    desc_details = "æè¿°ç”Ÿæˆå®Œæˆ"
                
                if progress_tracker:
                    progress_tracker.end_stage("å‡½æ•°æè¿°ç”Ÿæˆ (Description)", desc_details)
            else:
                result['error'] = "descriptionæ­¥éª¤å¤±è´¥"
                if progress_tracker:
                    progress_tracker.end_repo(repo_name, False)
                return result
            
            # Step 3: Vector Embedding
            if progress_tracker:
                progress_tracker.start_stage("å‘é‡åµŒå…¥ (Vector Embedding)")
            
            if self.run_vector_embedding(complete_file):
                result['steps']['vector_embedding'] = True
                if progress_tracker:
                    progress_tracker.end_stage("å‘é‡åµŒå…¥ (Vector Embedding)", "å‘é‡åµŒå…¥å®Œæˆ")
            else:
                result['error'] = "vector_embeddingæ­¥éª¤å¤±è´¥"
                if progress_tracker:
                    progress_tracker.end_repo(repo_name, False)
                return result
            
            # Step 4: Ingest
            if progress_tracker:
                progress_tracker.start_stage("æ•°æ®å¯¼å…¥ (Neo4j Ingest)")
            
            if self.run_ingest(complete_file):
                result['steps']['ingest'] = True
                result['success'] = True
                if progress_tracker:
                    progress_tracker.end_stage("æ•°æ®å¯¼å…¥ (Neo4j Ingest)", "æ•°æ®å¯¼å…¥Neo4jå®Œæˆ")
            else:
                result['error'] = "ingestæ­¥éª¤å¤±è´¥"
                if progress_tracker:
                    progress_tracker.end_repo(repo_name, False)
                return result
            
        except Exception as e:
            result['error'] = f"å¤„ç†å¼‚å¸¸: {str(e)}"
            logger.error(f"å¤„ç†ä»“åº“å¼‚å¸¸ {repo_name}: {e}")
            if progress_tracker:
                progress_tracker.end_repo(repo_name, False)
        
        finally:
            result['end_time'] = datetime.now().isoformat()
            if progress_tracker and result['success']:
                progress_tracker.end_repo(repo_name, True)
        
        return result
    
    def process_all_repos(self):
        """å¤„ç†æ‰€æœ‰ä»“åº?""
        # è¯»å–ä»“åº“é“¾æ¥
        repo_urls = self.read_repo_urls()
        self.stats['total_repos'] = len(repo_urls)
        self.stats['start_time'] = datetime.now().isoformat()
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç?{len(repo_urls)} ä¸ªä»“åº?)
        
        # å¤„ç†æ¯ä¸ªä»“åº“
        for i, repo_url in enumerate(repo_urls, 1):
            logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(repo_urls)}")
            
            result = self.process_single_repo(repo_url)
            self.stats['repo_results'].append(result)
            
            if result['success']:
                self.stats['successful_repos'] += 1
                logger.info(f"ä»“åº“å¤„ç†æˆåŠŸ: {result['repo_name']}")
            else:
                self.stats['failed_repos'] += 1
                logger.error(f"ä»“åº“å¤„ç†å¤±è´¥: {result['repo_name']} - {result['error']}")
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = (i / len(repo_urls)) * 100
            logger.info(f"æ€»ä½“è¿›åº¦: {progress:.1f}% ({i}/{len(repo_urls)})")
        
        self.stats['end_time'] = datetime.now().isoformat()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.save_statistics()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æ?        self.print_final_summary()
    
    def save_statistics(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = f"batch_processing_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    
    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦?""
        logger.info("=" * 60)
        logger.info("æ‰¹é‡å¤„ç†å®Œæˆï¼?)
        logger.info("=" * 60)
        logger.info(f"æ€»ä»“åº“æ•°: {self.stats['total_repos']}")
        logger.info(f"æˆåŠŸå¤„ç†: {self.stats['successful_repos']}")
        logger.info(f"å¤„ç†å¤±è´¥: {self.stats['failed_repos']}")
        logger.info(f"æˆåŠŸç? {(self.stats['successful_repos'] / self.stats['total_repos'] * 100):.1f}%")
        
        if self.stats['start_time'] and self.stats['end_time']:
            start = datetime.fromisoformat(self.stats['start_time'])
            end = datetime.fromisoformat(self.stats['end_time'])
            duration = end - start
            logger.info(f"æ€»è€—æ—¶: {duration}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„ä»“åº?        failed_repos = [r for r in self.stats['repo_results'] if not r['success']]
        if failed_repos:
            logger.info("\nå¤±è´¥çš„ä»“åº?")
            for repo in failed_repos:
                logger.info(f"  - {repo['repo_name']}: {repo['error']}")
        
        logger.info("=" * 60)


def main():
    """ä¸»å‡½æ•?""
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤„ç†å¤šä¸ªGitHubåº?)
    parser.add_argument('txt_file', help='åŒ…å«GitHubåº“é“¾æ¥çš„txtæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--neo4j-uri', default='bolt://localhost:7687', help='Neo4jæ•°æ®åº“URI')
    parser.add_argument('--neo4j-user', default='neo4j', help='Neo4jç”¨æˆ·å?)
    parser.add_argument('--neo4j-password', default='90879449Drq', help='Neo4jå¯†ç ')
    parser.add_argument('--neo4j-database', help='Neo4jæ•°æ®åº“å')
    parser.add_argument('--clean-db', action='store_true', help='æ¸…ç©ºæ•°æ®åº?)
    parser.add_argument('--filter-large-functions', action='store_true', help='è¿‡æ»¤å¤§å‡½æ•?)
    parser.add_argument('--concurrent', type=int, default=4, help='å¹¶å‘æ•?)
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹å¤„ç†å¤§å°?)
    parser.add_argument('--use-cache', action='store_true', default=True, help='ä½¿ç”¨ç¼“å­˜')
    parser.add_argument('--model-name', default='all-MiniLM-L6-v2', help='åµŒå…¥æ¨¡å‹åç§°')
    parser.add_argument('--model-type', default='sentence_transformers', help='åµŒå…¥æ¨¡å‹ç±»å‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™?    processor = BatchProcessor(
        txt_file=args.txt_file,
        neo4j_uri=args.neo4j_uri,
        neo4j_user=args.neo4j_user,
        neo4j_password=args.neo4j_password,
        neo4j_database=args.neo4j_database,
        clean_db=args.clean_db,
        filter_large_functions=args.filter_large_functions,
        concurrent=args.concurrent,
        batch_size=args.batch_size,
        use_cache=args.use_cache,
        model_name=args.model_name,
        model_type=args.model_type
    )
    
    # å¼€å§‹å¤„ç?    try:
        processor.process_all_repos()
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯? {e}")
        raise


if __name__ == "__main__":
    main()
