import logging
import json
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
import numpy as np

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    logging.warning("networkx not available, graph analysis will be limited")

from .base import BaseSearch
from .config import config

logger = logging.getLogger(__name__)

class GraphAnalysis(BaseSearch):
    """å›¾åˆ†æåŠŸèƒ½ç±»"""
    
    def __init__(self, 
                 neo4j_uri: Optional[str] = None,
                 neo4j_user: Optional[str] = None,
                 neo4j_password: Optional[str] = None,
                 neo4j_database: Optional[str] = None):
        """
        åˆå§‹åŒ–å›¾åˆ†æç±?
        
        Args:
            neo4j_uri: Neo4jæ•°æ®åº“URI
            neo4j_user: Neo4jç”¨æˆ·å?
            neo4j_password: Neo4jå¯†ç 
            neo4j_database: æ•°æ®åº“åç§?
        """
        super().__init__(neo4j_uri, neo4j_user, neo4j_password, neo4j_database)
        
        if not NETWORKX_AVAILABLE:
            logger.warning("NetworkXä¸å¯ç”¨ï¼Œéƒ¨åˆ†å›¾åˆ†æåŠŸèƒ½å°†è¢«ç¦ç”?)
        
        logger.info("å›¾åˆ†ææ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def calculate_centrality(self, 
                           algorithm: str = "pagerank",
                           top_k: int = 20,
                           include_weights: bool = True) -> Dict[str, Any]:
        """
        è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ?
        
        Args:
            algorithm: ä¸­å¿ƒæ€§ç®—æ³?("pagerank", "betweenness", "closeness", "eigenvector")
            top_k: è¿”å›å‰kä¸ªç»“æ?
            include_weights: æ˜¯å¦è€ƒè™‘è¾¹çš„æƒé‡
            
        Returns:
            ä¸­å¿ƒæ€§åˆ†æç»“æ?
        """
        logger.info(f"è®¡ç®—ä¸­å¿ƒæ€? {algorithm}, top_k: {top_k}")
        
        if not NETWORKX_AVAILABLE:
            return self._calculate_centrality_neo4j(algorithm, top_k)
        
        # æ„å»ºNetworkXå›?
        G = self._build_networkx_graph(include_weights)
        
        if G.number_of_nodes() == 0:
            logger.warning("å›¾ä¸­æ²¡æœ‰èŠ‚ç‚¹")
            return {"algorithm": algorithm, "results": [], "total_nodes": 0}
        
        # è®¡ç®—ä¸­å¿ƒæ€?
        if algorithm == "pagerank":
            centrality_scores = nx.pagerank(G, weight='weight' if include_weights else None)
        elif algorithm == "betweenness":
            centrality_scores = nx.betweenness_centrality(G, weight='weight' if include_weights else None)
        elif algorithm == "closeness":
            centrality_scores = nx.closeness_centrality(G, distance='weight' if include_weights else None)
        elif algorithm == "eigenvector":
            try:
                centrality_scores = nx.eigenvector_centrality(G, weight='weight' if include_weights else None)
            except nx.PowerIterationFailedConvergence:
                logger.warning("ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§è®¡ç®—æœªæ”¶æ•›ï¼Œä½¿ç”¨åº¦ä¸­å¿ƒæ€§æ›¿ä»?)
                centrality_scores = nx.degree_centrality(G)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¸­å¿ƒæ€§ç®—æ³? {algorithm}")
        
        # æ’åºå¹¶è¿”å›å‰kä¸ªç»“æ?
        sorted_scores = sorted(centrality_scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for i, (node_id, score) in enumerate(sorted_scores[:top_k]):
            # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
            node_info = self._get_node_info(node_id)
            results.append({
                "rank": i + 1,
                "node_id": node_id,
                "centrality_score": float(score),
                **node_info
            })
        
        return {
            "algorithm": algorithm,
            "results": results,
            "total_nodes": G.number_of_nodes(),
            "total_edges": G.number_of_edges(),
            "include_weights": include_weights
        }
    
    def _calculate_centrality_neo4j(self, algorithm: str, top_k: int) -> Dict[str, Any]:
        """ä½¿ç”¨Neo4jå†…ç½®ç®—æ³•è®¡ç®—ä¸­å¿ƒæ€?""
        logger.info(f"ä½¿ç”¨Neo4jè®¡ç®—ä¸­å¿ƒæ€? {algorithm}")
        
        if algorithm == "pagerank":
            query = f"""
            CALL gds.pageRank.stream('function-graph')
            YIELD nodeId, score
            RETURN gds.util.asNode(nodeId).id as node_id,
                   gds.util.asNode(nodeId).name as node_name,
                   gds.util.asNode(nodeId).filepath as filepath,
                   score
            ORDER BY score DESC
            LIMIT {top_k}
            """
        elif algorithm == "betweenness":
            query = f"""
            CALL gds.betweenness.stream('function-graph')
            YIELD nodeId, score
            RETURN gds.util.asNode(nodeId).id as node_id,
                   gds.util.asNode(nodeId).name as node_name,
                   gds.util.asNode(nodeId).filepath as filepath,
                   score
            ORDER BY score DESC
            LIMIT {top_k}
            """
        else:
            # å›é€€åˆ°ç®€å•çš„åº¦ä¸­å¿ƒæ€?
            query = f"""
            MATCH (fn:Function)
            OPTIONAL MATCH (fn)-[r:INTERNAL_CALLS|CALLS]->()
            WITH fn, count(r) as out_degree
            OPTIONAL MATCH ()-[r:INTERNAL_CALLS|CALLS]->(fn)
            WITH fn, out_degree, count(r) as in_degree
            WITH fn, out_degree + in_degree as total_degree
            RETURN fn.id as node_id,
                   fn.name as node_name,
                   fn.filepath as filepath,
                   total_degree as score
            ORDER BY total_degree DESC
            LIMIT {top_k}
            """
        
        try:
            results = self._run_query(query)
            formatted_results = []
            for i, record in enumerate(results):
                formatted_results.append({
                    "rank": i + 1,
                    "node_id": record.get("node_id"),
                    "centrality_score": float(record.get("score", 0)),
                    "node_name": record.get("node_name"),
                    "filepath": record.get("filepath")
                })
            
            return {
                "algorithm": algorithm,
                "results": formatted_results,
                "total_nodes": len(results),
                "include_weights": False
            }
        except Exception as e:
            logger.error(f"Neo4jä¸­å¿ƒæ€§è®¡ç®—å¤±è´? {e}")
            return {"algorithm": algorithm, "results": [], "error": str(e)}
    
    def _build_networkx_graph(self, include_weights: bool = True) -> 'nx.DiGraph':
        """æ„å»ºNetworkXå›?""
        G = nx.DiGraph()
        
        # è·å–æ‰€æœ‰èŠ‚ç‚?
        nodes_query = """
        MATCH (fn:Function)
        RETURN fn.id as id, fn.name as name, fn.filepath as filepath,
               fn.complexity_level as complexity_level,
               fn.cyclomatic_complexity as cyclomatic_complexity
        """
        
        nodes = self._run_query(nodes_query)
        for node in nodes:
            G.add_node(
                node["id"],
                name=node["name"],
                filepath=node["filepath"],
                complexity_level=node["complexity_level"],
                cyclomatic_complexity=node["cyclomatic_complexity"]
            )
        
        # è·å–æ‰€æœ‰è¾¹
        edges_query = """
        MATCH (source:Function)-[r:INTERNAL_CALLS|CALLS]->(target:Function)
        RETURN source.id as source, target.id as target, type(r) as relationship_type
        """
        
        edges = self._run_query(edges_query)
        for edge in edges:
            weight = 1.0
            if include_weights:
                # å¯ä»¥æ ¹æ®å…³ç³»ç±»å‹è®¾ç½®ä¸åŒæƒé‡
                if edge["relationship_type"] == "INTERNAL_CALLS":
                    weight = 1.0
                elif edge["relationship_type"] == "CALLS":
                    weight = 0.5
            
            G.add_edge(
                edge["source"],
                edge["target"],
                weight=weight,
                relationship_type=edge["relationship_type"]
            )
        
        logger.info(f"æ„å»ºNetworkXå›¾å®Œæˆ? {G.number_of_nodes()}ä¸ªèŠ‚ç‚? {G.number_of_edges()}æ¡è¾¹")
        return G
    
    def _get_node_info(self, node_id: str) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯"""
        query = """
        MATCH (fn:Function {id: $node_id})
        RETURN fn.name as node_name,
               fn.filepath as filepath,
               fn.complexity_level as complexity_level,
               fn.cyclomatic_complexity as cyclomatic_complexity,
               fn.lines_of_code as lines_of_code
        """
        
        result = self._run_query_single(query, {"node_id": node_id})
        return dict(result) if result else {}
    
    def find_communities(self, 
                        algorithm: str = "louvain",
                        min_community_size: int = 2,
                        resolution: float = 1.0) -> Dict[str, Any]:
        """
        ç¤¾åŒºå‘ç°
        
        Args:
            algorithm: ç¤¾åŒºå‘ç°ç®—æ³• ("louvain", "leiden", "label_propagation")
            min_community_size: æœ€å°ç¤¾åŒºå¤§å°?
            resolution: åˆ†è¾¨ç‡å‚æ•°ï¼ˆä»…å¯¹louvainå’Œleidenæœ‰æ•ˆï¼?
            
        Returns:
            ç¤¾åŒºå‘ç°ç»“æœ
        """
        logger.info(f"ç¤¾åŒºå‘ç°: {algorithm}, æœ€å°ç¤¾åŒºå¤§å°? {min_community_size}")
        
        if not NETWORKX_AVAILABLE:
            return self._find_communities_neo4j(algorithm, min_community_size)
        
        # æ„å»ºæ— å‘å›¾ï¼ˆç¤¾åŒºå‘ç°é€šå¸¸åœ¨æ— å‘å›¾ä¸Šè¿›è¡Œï¼‰
        G = self._build_networkx_graph(include_weights=True)
        G_undirected = G.to_undirected()
        
        if G_undirected.number_of_nodes() == 0:
            logger.warning("å›¾ä¸­æ²¡æœ‰èŠ‚ç‚¹")
            return {"algorithm": algorithm, "communities": [], "total_nodes": 0}
        
        # æ‰§è¡Œç¤¾åŒºå‘ç°
        if algorithm == "louvain":
            try:
                import networkx.algorithms.community as nx_comm
                communities = nx_comm.louvain_communities(G_undirected, weight='weight', resolution=resolution)
            except ImportError:
                logger.warning("louvainç®—æ³•ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡ç­¾ä¼ æ’­")
                communities = list(nx_comm.label_propagation_communities(G_undirected))
        elif algorithm == "leiden":
            try:
                import leidenalg
                import igraph as ig
                # è½¬æ¢ä¸ºigraphæ ¼å¼
                ig_graph = ig.Graph.from_networkx(G_undirected)
                partition = leidenalg.find_partition(ig_graph, leidenalg.ModularityVertexPartition, resolution_parameter=resolution)
                communities = [list(partition[i]) for i in range(len(partition))]
            except ImportError:
                logger.warning("leidenç®—æ³•ä¸å¯ç”¨ï¼Œä½¿ç”¨louvain")
                import networkx.algorithms.community as nx_comm
                communities = nx_comm.louvain_communities(G_undirected, weight='weight')
        elif algorithm == "label_propagation":
            import networkx.algorithms.community as nx_comm
            communities = list(nx_comm.label_propagation_communities(G_undirected))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç¤¾åŒºå‘ç°ç®—æ³•: {algorithm}")
        
        # è¿‡æ»¤å°ç¤¾åŒ?
        filtered_communities = [comm for comm in communities if len(comm) >= min_community_size]
        
        # æ ¼å¼åŒ–ç»“æ?
        community_results = []
        for i, community in enumerate(filtered_communities):
            community_info = {
                "community_id": i + 1,
                "size": len(community),
                "nodes": []
            }
            
            for node_id in community:
                node_info = self._get_node_info(node_id)
                community_info["nodes"].append({
                    "node_id": node_id,
                    **node_info
                })
            
            community_results.append(community_info)
        
        # æŒ‰ç¤¾åŒºå¤§å°æ’åº?
        community_results.sort(key=lambda x: x["size"], reverse=True)
        
        return {
            "algorithm": algorithm,
            "communities": community_results,
            "total_communities": len(community_results),
            "total_nodes": G_undirected.number_of_nodes(),
            "min_community_size": min_community_size,
            "resolution": resolution
        }
    
    def _find_communities_neo4j(self, algorithm: str, min_community_size: int) -> Dict[str, Any]:
        """ä½¿ç”¨Neo4jå†…ç½®ç®—æ³•è¿›è¡Œç¤¾åŒºå‘ç°"""
        logger.info(f"ä½¿ç”¨Neo4jè¿›è¡Œç¤¾åŒºå‘ç°: {algorithm}")
        
        if algorithm == "louvain":
            query = """
            CALL gds.louvain.stream('function-graph')
            YIELD nodeId, communityId
            RETURN gds.util.asNode(nodeId).id as node_id,
                   gds.util.asNode(nodeId).name as node_name,
                   gds.util.asNode(nodeId).filepath as filepath,
                   communityId
            ORDER BY communityId, node_name
            """
        else:
            # å›é€€åˆ°å¼±è¿é€šåˆ†é‡?
            query = """
            CALL gds.wcc.stream('function-graph')
            YIELD nodeId, componentId
            RETURN gds.util.asNode(nodeId).id as node_id,
                   gds.util.asNode(nodeId).name as node_name,
                   gds.util.asNode(nodeId).filepath as filepath,
                   componentId
            ORDER BY componentId, node_name
            """
        
        try:
            results = self._run_query(query)
            
            # æŒ‰ç¤¾åŒºåˆ†ç»?
            communities_dict = defaultdict(list)
            for record in results:
                community_id = record["componentId"] if algorithm != "louvain" else record["communityId"]
                communities_dict[community_id].append({
                    "node_id": record["node_id"],
                    "node_name": record["node_name"],
                    "filepath": record["filepath"]
                })
            
            # è¿‡æ»¤å°ç¤¾åŒºå¹¶æ ¼å¼åŒ?
            community_results = []
            for i, (community_id, nodes) in enumerate(communities_dict.items()):
                if len(nodes) >= min_community_size:
                    community_results.append({
                        "community_id": i + 1,
                        "size": len(nodes),
                        "nodes": nodes
                    })
            
            # æŒ‰ç¤¾åŒºå¤§å°æ’åº?
            community_results.sort(key=lambda x: x["size"], reverse=True)
            
            return {
                "algorithm": algorithm,
                "communities": community_results,
                "total_communities": len(community_results),
                "total_nodes": len(results),
                "min_community_size": min_community_size
            }
        except Exception as e:
            logger.error(f"Neo4jç¤¾åŒºå‘ç°å¤±è´¥: {e}")
            return {"algorithm": algorithm, "communities": [], "error": str(e)}
    
    def calculate_similarity_matrix(self, 
                                  function_names: List[str],
                                  similarity_type: str = "structural") -> Dict[str, Any]:
        """
        è®¡ç®—å‡½æ•°é—´çš„ç›¸ä¼¼åº¦çŸ©é˜?
        
        Args:
            function_names: å‡½æ•°ååˆ—è¡?
            similarity_type: ç›¸ä¼¼åº¦ç±»å?("structural", "semantic", "jaccard")
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜?
        """
        logger.info(f"è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜? {len(function_names)}ä¸ªå‡½æ•? ç±»å‹: {similarity_type}")
        
        if similarity_type == "structural":
            return self._calculate_structural_similarity(function_names)
        elif similarity_type == "semantic":
            return self._calculate_semantic_similarity(function_names)
        elif similarity_type == "jaccard":
            return self._calculate_jaccard_similarity(function_names)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦ç±»å? {similarity_type}")
    
    def _calculate_structural_similarity(self, function_names: List[str]) -> Dict[str, Any]:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº?""
        # è·å–å‡½æ•°çš„è°ƒç”¨å…³ç³?
        functions_data = {}
        for func_name in function_names:
            # è·å–å‡½æ•°çš„è°ƒç”¨å’Œè¢«è°ƒç”¨å…³ç³?
            query = """
            MATCH (fn:Function {name: $func_name})
            OPTIONAL MATCH (fn)-[:INTERNAL_CALLS|CALLS]->(callee:Function)
            OPTIONAL MATCH (caller:Function)-[:INTERNAL_CALLS|CALLS]->(fn)
            RETURN fn.name as name,
                   collect(DISTINCT callee.name) as callees,
                   collect(DISTINCT caller.name) as callers,
                   fn.cyclomatic_complexity as complexity,
                   fn.lines_of_code as lines
            """
            
            result = self._run_query_single(query, {"func_name": func_name})
            if result:
                functions_data[func_name] = {
                    "callees": set(result["callees"]) if result["callees"] else set(),
                    "callers": set(result["callers"]) if result["callers"] else set(),
                    "complexity": result["complexity"] or 0,
                    "lines": result["lines"] or 0
                }
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜?
        n = len(function_names)
        similarity_matrix = np.zeros((n, n))
        
        for i, func1 in enumerate(function_names):
            for j, func2 in enumerate(function_names):
                if i == j:
                    similarity_matrix[i][j] = 1.0
                elif func1 in functions_data and func2 in functions_data:
                    data1 = functions_data[func1]
                    data2 = functions_data[func2]
                    
                    # è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼ˆåŸºäºè°ƒç”¨å…³ç³»ï¼?
                    callees_intersection = len(data1["callees"] & data2["callees"])
                    callees_union = len(data1["callees"] | data2["callees"])
                    callers_intersection = len(data1["callers"] & data2["callers"])
                    callers_union = len(data1["callers"] | data2["callers"])
                    
                    callees_sim = callees_intersection / callees_union if callees_union > 0 else 0
                    callers_sim = callers_intersection / callers_union if callers_union > 0 else 0
                    
                    # å¤æ‚åº¦ç›¸ä¼¼åº¦
                    complexity_sim = 1 - abs(data1["complexity"] - data2["complexity"]) / max(data1["complexity"], data2["complexity"], 1)
                    
                    # ç»¼åˆç›¸ä¼¼åº?
                    similarity_matrix[i][j] = (callees_sim + callers_sim + complexity_sim) / 3
        
        return {
            "similarity_type": "structural",
            "function_names": function_names,
            "similarity_matrix": similarity_matrix.tolist(),
            "functions_data": {name: {
                "callees": list(data["callees"]),
                "callers": list(data["callers"]),
                "complexity": data["complexity"],
                "lines": data["lines"]
            } for name, data in functions_data.items()}
        }
    
    def _calculate_semantic_similarity(self, function_names: List[str]) -> Dict[str, Any]:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆéœ€è¦åµŒå…¥æ¨¡å‹ï¼‰"""
        # è¿™é‡Œéœ€è¦ç»“åˆSemanticSearchç±»çš„åŠŸèƒ½
        # æš‚æ—¶è¿”å›ç»“æ„ç›¸ä¼¼åº?
        logger.warning("è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—éœ€è¦åµŒå…¥æ¨¡å‹ï¼Œå›é€€åˆ°ç»“æ„ç›¸ä¼¼åº¦")
        return self._calculate_structural_similarity(function_names)
    
    def _calculate_jaccard_similarity(self, function_names: List[str]) -> Dict[str, Any]:
        """è®¡ç®—Jaccardç›¸ä¼¼åº?""
        # è·å–æ¯ä¸ªå‡½æ•°çš„ç‰¹å¾é›†å?
        functions_features = {}
        for func_name in function_names:
            query = """
            MATCH (fn:Function {name: $func_name})
            OPTIONAL MATCH (fn)-[:INTERNAL_CALLS|CALLS]->(callee:Function)
            RETURN fn.name as name,
                   collect(DISTINCT callee.name) as callees
            """
            
            result = self._run_query_single(query, {"func_name": func_name})
            if result:
                functions_features[func_name] = set(result["callees"]) if result["callees"] else set()
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦çŸ©é˜?
        n = len(function_names)
        similarity_matrix = np.zeros((n, n))
        
        for i, func1 in enumerate(function_names):
            for j, func2 in enumerate(function_names):
                if i == j:
                    similarity_matrix[i][j] = 1.0
                elif func1 in functions_features and func2 in functions_features:
                    set1 = functions_features[func1]
                    set2 = functions_features[func2]
                    
                    intersection = len(set1 & set2)
                    union = len(set1 | set2)
                    
                    similarity_matrix[i][j] = intersection / union if union > 0 else 0
        
        return {
            "similarity_type": "jaccard",
            "function_names": function_names,
            "similarity_matrix": similarity_matrix.tolist(),
            "functions_features": {name: list(features) for name, features in functions_features.items()}
        }
    
    def get_graph_statistics(self) -> Dict[str, Any]:
        """è·å–å›¾ç»Ÿè®¡ä¿¡æ?""
        logger.info("è·å–å›¾ç»Ÿè®¡ä¿¡æ?)
        
        # åŸºæœ¬ç»Ÿè®¡
        basic_stats_query = """
        MATCH (fn:Function)
        OPTIONAL MATCH (fn)-[r:INTERNAL_CALLS|CALLS]->()
        WITH fn, count(r) as out_degree
        OPTIONAL MATCH ()-[r:INTERNAL_CALLS|CALLS]->(fn)
        WITH fn, out_degree, count(r) as in_degree
        RETURN count(fn) as total_nodes,
               sum(out_degree) as total_edges,
               avg(out_degree + in_degree) as avg_degree,
               max(out_degree + in_degree) as max_degree,
               min(out_degree + in_degree) as min_degree
        """
        
        basic_stats = self._run_query_single(basic_stats_query)
        
        # è¿é€šæ€§ç»Ÿè®?
        connectivity_query = """
        CALL gds.wcc.stream('function-graph')
        YIELD componentId
        WITH componentId, count(*) as component_size
        RETURN count(componentId) as total_components,
               max(component_size) as largest_component_size,
               avg(component_size) as avg_component_size
        """
        
        try:
            connectivity_stats = self._run_query_single(connectivity_query)
        except Exception as e:
            logger.warning(f"è¿é€šæ€§ç»Ÿè®¡å¤±è´? {e}")
            connectivity_stats = {}
        
        # å¤æ‚åº¦åˆ†å¸?
        complexity_query = """
        MATCH (fn:Function)
        WHERE fn.complexity_level IS NOT NULL
        RETURN fn.complexity_level as level, count(fn) as count
        ORDER BY count DESC
        """
        
        complexity_dist = self._run_query(complexity_query)
        
        return {
            "basic_statistics": dict(basic_stats) if basic_stats else {},
            "connectivity_statistics": dict(connectivity_stats) if connectivity_stats else {},
            "complexity_distribution": [dict(record) for record in complexity_dist],
            "analysis_timestamp": str(pd.Timestamp.now()) if 'pd' in globals() else None
        }
